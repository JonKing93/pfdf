{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed57bc1f-f996-4f11-bfab-86e48d2968ea",
   "metadata": {},
   "source": [
    "# Parameter Sweep Tutorial\n",
    "This tutorial demonstrates how to run the M1 likelihood model using multiple values of the model's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb50f86-bc56-4242-9efd-b4a7cc5a6287",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The models in pfdf allow users to specify the calibration parameters. By default, the models will use the parameters described in the scientific literature, but advanced users may wish to use different parameters in some cases. One anticipated use case is testing multiple values of model parameters in order to calibrate models to new regions. For example, the models of [Staley et al., 2017](https://doi.org/10.1016/j.geomorph.2016.10.019) were calibrated in southern California, but these models are often applied in other regions - such as Arizona and Colorado. As such, researchers may be interested in running these models with a suite of parameters, and pfdf provides support for this.\n",
    "\n",
    "In this tutorial, we will examine how to run assessment models with multiple parameters, using the M1 model of Staley et al. (2017) as an example. We'll examine how to run the model using multiple values of the same parameter, and then an advanced case of runs using multiple varying parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b8954-b61b-4d72-9fb6-59dd00e0f823",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23587718-ddb0-4918-adc9-510d4fc2f9d5",
   "metadata": {},
   "source": [
    "### Install pfdf\n",
    "To run this tutorial, you must have installed [pfdf 3+ with tutorial resources](https://ghsc.code-pages.usgs.gov/lhp/pfdf/resources/installation.html#tutorials) in your Jupyter kernel. The following line checks this is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b46bf4c-fc25-40da-b304-73c4a4bed6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import check_installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5ff80-1867-45f0-ba2a-4fd878b735b8",
   "metadata": {},
   "source": [
    "### Previous Tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41878d34-555f-48f6-b141-9686a57642e1",
   "metadata": {},
   "source": [
    "You must run the [Preprocessing Tutorial](04_Preprocessing.ipynb) before this one. This is because we'll use the preprocessed datasets to derive a stream segment network for this tutorial. The following line checks the workspace for the preprocessed datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e9bac7-a5b4-4803-b000-904bc1cc39ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import workspace\n",
    "workspace.check_preprocessed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b778c-334d-450f-9f58-07480e8b02ae",
   "metadata": {},
   "source": [
    "We also **strongly recommend** completing the [Hazard Assessment Tutorial](05_Hazard_Assessment.ipynb) before this one. This is because this tutorial assumes familiarity with many of the concepts introduced in that tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83ca025-1549-49cc-9e6b-900a2aed030e",
   "metadata": {},
   "source": [
    "### Example Network\n",
    "Next, we'll build an example stream segment network. This process is explored in detail in the [Hazard Assessment Tutorial](05_Hazard_Assessment.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcdb59c-9502-46d1-a3aa-15d5685b596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import examples\n",
    "segments = examples.build_segments()\n",
    "print(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c760b1ec-8c65-4449-8d01-5009ce3e5835",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Finally, we'll import the `s17` module, which implements the Staley 2017 models. We'll also import `numpy`, to help work with parameter arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67fcbdb-c022-47d8-b641-89422c76c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfdf.models import s17\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731abd8a-01f6-468c-836f-e9e6234a6d5e",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd6600b-f30b-42cd-af89-776dd47d6c00",
   "metadata": {},
   "source": [
    "### Model Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca37539c-f67e-49ac-9f1c-10cf5847b66c",
   "metadata": {},
   "source": [
    "We'll start by computing the (T)errain, (F)ire, and (S)oil variables for the M1 model. As discussed in the hazard assessment tutorial, the final `T`, `F`, and `S` outputs are 1D arrays with one element per stream segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14942d54-8e97-4de6-9220-e4f037a355a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfdf.raster import Raster\n",
    "from pfdf import severity, watershed\n",
    "\n",
    "# Load datasets\n",
    "dem = Raster.from_file('preprocessed/dem.tif')\n",
    "dnbr = Raster.from_file('preprocessed/dnbr.tif')\n",
    "kf = Raster.from_file('preprocessed/kf.tif')\n",
    "barc4 = Raster.from_file('preprocessed/barc4.tif')\n",
    "\n",
    "# Determine watershed characteristics\n",
    "moderate_high = severity.mask(barc4, [\"moderate\", \"high\"])\n",
    "conditioned = watershed.condition(dem)\n",
    "flow = watershed.flow(conditioned)\n",
    "slopes = watershed.slopes(conditioned, flow)\n",
    "\n",
    "# Compute M1 variables\n",
    "T, F, S = s17.M1.variables(segments, moderate_high, slopes, dnbr, kf, omitnan=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce439f1-b031-4280-a8df-ac125293dbf0",
   "metadata": {},
   "source": [
    "In the hazard assessment tutorial, we saw how to run the M1 model on these variable using the standard calibration parameters from the literature. In this tutorial, we'll still use these variables, but we'll run the model with different calibration parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519ef8a-0bad-45dd-85b0-391763bc2048",
   "metadata": {},
   "source": [
    "### Design Storms\n",
    "We'll also define some example design storms for the model. For simplicity, we'll only use design storms for 15-minute rainfall durations. As a reminder, the design storms are peak rainfall accumulations (in millimeters) over a 15-minute interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97886972-c008-4844-a0ce-140ac0046146",
   "metadata": {},
   "outputs": [],
   "source": [
    "R15 = [5, 6, 8.75, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90433a37-3f0d-48f1-94ac-c2668738a751",
   "metadata": {},
   "source": [
    "## Testing One Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f17bdcf-e648-4416-87a4-4d4cc55bee4c",
   "metadata": {},
   "source": [
    "As a reminder, the M1 model uses 4 calibrated coefficients in its logistic regression model. These are:\n",
    "\n",
    "| Coefficient | Description |\n",
    "| --------- | ----------- |\n",
    "| B | Model intercept |\n",
    "| Ct | Terrain variable coefficient |\n",
    "| Cf | Fire variable coefficient |\n",
    "| Cs | Soil variable coefficient |\n",
    "\n",
    "Let's say we'd like to calibrate the model's `Ct` parameter for our dataset. One approach could be to run the model using multiple values of `Ct`. If we have a database of known debris-flow events, we could compare the database to the model results to try and determine an optimal `Ct` value.\n",
    "\n",
    "Here, we'll demonstrate how to run the model using multiple `Ct` values. (Comparing results to a database is beyond the scope of this tutorial). For brevity, we'll limit our investigation to 15-minute rainfall durations. We'll start by getting the standard `B`, `Cf`, and `Cs` variables from the literature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f878020-263c-4480-b5bc-66c662565a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, _, Cf, Cs = s17.M1.parameters(durations=15)\n",
    "print(B)\n",
    "print(Cf)\n",
    "print(Cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9743db-b1c4-4cf1-849d-891569e917e6",
   "metadata": {},
   "source": [
    "Next, we'll sample every `Ct` value from 0.01 to 1 in steps of 0.01. Examining the shape, we note we've sampled 100 possible values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d84e5a7-96e5-452e-ae0a-176505aee74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ct = np.arange(0.01, 1.01, 0.01)\n",
    "print(Ct.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc2e18-3dfa-40c7-b1be-4d4deddbfe7a",
   "metadata": {},
   "source": [
    "Finally, we'll run the likelihood model using our sampled parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f24b07-0726-4001-ac7c-a5c383808daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoods = s17.likelihood(R15, B, Ct, T, Cf, F, Cs, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a7a1c6-cebc-4493-912d-da7d12d85e52",
   "metadata": {},
   "source": [
    "The output is a 3D numpy array. We previously discussed that each row holds likelihoods for a stream segment, and each column is a design storm. However, the third dimension is new. Here, each element along the third dimension holds results for a unique set of calibration parameters. We will refer to these sets as model **runs**.\n",
    "\n",
    "For example, examining the shape of the output array, we find it has 100 elements along the third dimension. Each of these elements corresponds to one of our 100 sampled `Ct` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c8f1a2-59f7-439b-b1f3-f0be462f517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(likelihoods.shape)\n",
    "print((segments.size, len(R15), Ct.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91366246-d328-4f57-a8eb-165639565ad4",
   "metadata": {},
   "source": [
    "We can now go on to compare these results to a database of debris-flow events, or use the results for other research purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d196598-2a01-4039-85dd-864651a745c3",
   "metadata": {},
   "source": [
    "## Testing Multiple Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4797ba-878b-4efe-bb16-aa014b2d48c8",
   "metadata": {},
   "source": [
    "The previous example only sampled one parameter. However, we could instead choose to test multiple values of multiple parameters simultaneously. When this is the case, all parameters with multiple values should be vectors with the same number of elements. Each iterative set of parameter values is then used for a distinct model run. Any scalar parameters will use the same value for each run.\n",
    "\n",
    "For example, let's say we've decided to sample the `Ct` and `Cf` parameters simultaneously. We'll generate 1000 random values of `Ct` with a mean of 0.4 and standard deviation of 0.25. We'll also generate 1000 values of `Cf` with a mean of 0.67 and a standard deviation of 0.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab29a5-3a7f-408e-bd24-f1a65ccb22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeds the random number generator to make this example reproducible\n",
    "rng = np.random.default_rng(seed=123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a1e7c8-e7ca-4560-9dad-0662fa6594fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the parameters\n",
    "Ct = 0.4 + 0.25 * np.random.rand(1000)\n",
    "Cf = 0.67 + 0.3 * np.random.rand(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b2c6a-ee0b-4d3f-91f3-55ed640f228c",
   "metadata": {},
   "source": [
    "We can now run the model as usual. Note that we're using the scalar `B` and `Cs` values from the literature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b2e7a2-d844-4be6-9181-bc068daf2a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoods = s17.likelihood(R15, B, Ct, T, Cf, F, Cs, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f4cf7b-b889-4b56-846a-6059ce9341a6",
   "metadata": {},
   "source": [
    "Once again, the output is a 3D array. Examining the shape, we note there are 1000 elements along the third dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8bee08-b43d-48da-be01-a156660b4a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(likelihoods.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d2afd-91f3-446f-8287-62b124629465",
   "metadata": {},
   "source": [
    "Here, each element along the third dimension holds the likelihoods for one of the 1000 tested `(Ct, Cf)` pairs. For example, element 0 holds the results for `(Ct[0], Cf[0])`, column 1 holds results for `(Ct[1], Cf[1])`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec8e70f-58ad-4710-8e14-acfc1ca3bdd4",
   "metadata": {},
   "source": [
    "### Important!\n",
    "This tutorial used random sampling for the sake of brevity, but this is often a poor sampling method in practice, as it can result in undersampled regions of the parameter space. Instead, we recommend users consider more sophisticated sampling methods, such as [Latin Hypercube Sampling](https://en.wikipedia.org/wiki/Latin_hypercube_sampling)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b396d6a4-5f56-4f30-9617-b51edbdfa6e7",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c227703-62a5-4427-808e-3d75b9811f94",
   "metadata": {},
   "source": [
    "In this tutorial, we've learned how to run a hazard assessment model using new model parameters, using the M1 likelihood model as an example. In the tutorial, we saw how to run the model for (1) a single sampled parameter, and (2) multiple parameters sampled simultaneously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}