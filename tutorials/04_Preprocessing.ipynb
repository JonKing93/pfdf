{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504141d5-07d4-468d-91c6-d31a0c5a47c7",
   "metadata": {},
   "source": [
    "# Preprocessing Tutorial\n",
    "This tutorial demonstrates how to use ``Raster`` routines to clean and reproject datasets for an assessment. Or [skip to the end](#Quick-Reference) for a quick reference script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab88cee2-84b5-4ce2-9cb4-575271b5548c",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf492d-ec75-42b6-9573-db480dcbabcb",
   "metadata": {},
   "source": [
    "Many pfdf routines use multiple raster datasets as input. When this is the case, pfdf usually requires the rasters to have the same shape, resolution, CRS, and alignment. Some routines also require data values to fall within a valid data range. However, real datasets are rarely this clean, and so the ``Raster`` class provides commands to help achieve these requirements.\n",
    "\n",
    "In this tutorial, we'll apply preprocessing routines to the input rasters for a real hazard assessment - the 2016 San Gabriel Fire Complex in California."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe16bae-ec00-477b-99c2-70927769c359",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cca30a-4b2e-46ea-bcd6-161b0b82c2a4",
   "metadata": {},
   "source": [
    "### Install pfdf\n",
    "To run this tutorial, you must have installed [pfdf 3+ with tutorial resources](https://ghsc.code-pages.usgs.gov/lhp/pfdf/resources/installation.html#tutorials) in your Jupyter kernel. The following line checks this is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6243ac1-3a29-446e-ba13-d5d35f536e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import check_installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102a0c2b-9ce1-4780-acab-dc4c46030ed4",
   "metadata": {},
   "source": [
    "### Download Data Tutorial\n",
    "This tutorial uses datasets that are downloaded in the [Download Data Tutorial](03_Download_Data.ipynb). If you have not run that tutorial, then you should do so now. The following line checks the required datasets have been downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab8ffad-9735-4a00-a914-adc37a3bde5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import workspace\n",
    "workspace.check_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbf340c-e926-420e-86b7-c3bc5fc48925",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Next, we'll import the `Raster` class, which we'll use to load and preprocess the datasets. We'll also import `pathlib.Path`, which we'll use to manipulate file paths; and the `plot` module, which we'll use to visualize the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e3e16-f8fe-44a7-8b5f-703bc46d0d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfdf.raster import Raster\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tools import plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742bff08-e11d-41a9-aaec-c3ebd0165b50",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea411fc3-3182-41cb-9db2-996afc6415b5",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "In this tutorial we'll preprocess the following datasets:\n",
    "\n",
    "| Dataset | Description | Type | EPSG | Resolution | Source |\n",
    "| :------ | :---------- | :--- | :--- | :--------- | :----- |\n",
    "| perimeter | Fire burn perimeter | MultiPolygon | 26911 | NA | User  provided |\n",
    "| dem | Digital elevation model (10 meter resolution) | Raster | 4269 | 10 meter | `data.usgs.tnm.dem` |\n",
    "| dnbr | Differenced normalized burn ratio | Raster | 26911 | 10 meter | User provided |\n",
    "| kf | Soil KF-factors | Raster | 5069 | 30 meter | `data.usgs.statsgo` |\n",
    "| evt | Existing vegetation type | Raster | 5070 | 30 meter | `data.landfire` |\n",
    "| retainments | Debris retainment features | MultiPoint | 4326 | NA | `data.retainments.la_county` |\n",
    "\n",
    "As can be seen from the table, these datasets are in a variety of formats (Raster, Polygons, Points), use 5 different CRS, and have several different resolutions. The aim of this tutorial is to convert these datasets to rasters with the same CRS, resolution, and bounding box. Specifically, the preprocessed rasters will have the following characteristics:\n",
    "\n",
    "| Characteristic | Value | Note |\n",
    "| :------------- | :---- | :--- |\n",
    "| EPSG | 4269 | Matches the DEM |\n",
    "| Resolution | 10 meter | Hazard models were calibrated using 10 meter DEMs |\n",
    "| Bounding Box |  | Will match the buffered fire perimeter |\n",
    "\n",
    "We'll also ensure that certain datasets (`dnbr` and `kf`) fall within valid data ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977f11b8-cc9c-43e6-a78e-f43c238dde7c",
   "metadata": {},
   "source": [
    "### Derived\n",
    "We'll also produce the following new datasets from the inputs:\n",
    "\n",
    "| Derived Dataset | Description | Source |\n",
    "| :-------------- | :---------- | :----- |\n",
    "| severity | Soil burn severity | Estimated from dNBR |\n",
    "| iswater | Water body mask | Derived from EVT |\n",
    "| isdeveloped | Human development mask | Derived from EVT |\n",
    "\n",
    "These datasets are typically considered inputs to the hazard assessment process, hence their inclusion in the preprocessing stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f4d453-a1f0-4c0c-b39a-e0ee1b08b712",
   "metadata": {},
   "source": [
    "#### Important!\n",
    "Hazard assessments should use field validated soil burn severity whenever possible. We use a derived value here to demonstrate how this dataset can be estimated when field-validated data is not available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da3e2c4-2240-425b-a5a2-11e9a6d0b36c",
   "metadata": {},
   "source": [
    "## Acquire Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddf2c14-33aa-4be8-afc9-0246fd6e14f8",
   "metadata": {},
   "source": [
    "In many hazard assessments, you'll need to provide the fire perimeter and dNBR raster. You'll also often provide the soil burn severity, but we've deliberately neglected that dataset in this tutorial. \n",
    "\n",
    "As we saw in the data tutorial, the analysis domain is typically defined by a buffered fire perimeter. The perimeter is buffered so that the assessment will include areas downstream of the burn area. Let's start by loading the buffered burn perimeter and dNBR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c927f472-c63c-4164-958c-a64399833050",
   "metadata": {},
   "outputs": [],
   "source": [
    "perimeter = Raster.from_file('data/buffered-perimeter.tif')\n",
    "dnbr = Raster.from_file('data/dnbr.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b824c-dac0-4d95-9527-0ff5d2011f5b",
   "metadata": {},
   "source": [
    "In the data tutorial, we also saw how you can use `pfdf.data` to download several commonly-used raster datasets. These included USGS DEMs, STATSGO soil KF-factors, and LANDFIRE EVTs. We already downloaded these datasets in the data tutorial, so let's load them now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb52f43-a7eb-42ba-b650-dca95dc18db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem = Raster.from_file('data/dem.tif')\n",
    "evt = Raster.from_file('data/evt.tif')\n",
    "kf = Raster.from_file('data/kf.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7970b991-9c57-4a56-88e2-4e07715e578d",
   "metadata": {},
   "source": [
    "We also used `pfdf.data` to download several non-raster datasets. Of these, we're particularly interested in the debris retainment features, as hazard assessments can use these to design the stream segment network. The downloaded debris retainment features are a Point feature collection - this is not a raster dataset, so we'll just get the path to the dataset for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf050a-ebd5-49d2-8989-fe63162de6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retainments_path = 'data/la-county-retainments.gdb'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f064949-5d55-4f37-9d11-f1e8537279db",
   "metadata": {},
   "source": [
    "## Rasterize Debris Retainments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ca6b15-e9e5-44ac-8bef-c7e2f249dc70",
   "metadata": {},
   "source": [
    "Hazard assessments require most datasets to be rasters, so we'll need to rasterize any vector feature datasets before proceeding. The debris retainments are vector features - specifically, a collection of Point geometries - so we'll rasterize them here.\n",
    "\n",
    "You can convert Point or MultiPoint features to a `Raster` object using the `from_points` factory. We'll only introduce the factory here, but you can find a detailed discussion in the [Raster Factories Tutorial](07_Raster_Factories.ipynb). In brief, this command takes the path to a Point feature file, and converts the dataset to a raster. By default, the rasterized dataset will have a resolution of 10 meters, which is the recommended resolution for standard hazard assessments.\n",
    "\n",
    "Before calling the `from_points` command, let's compare the retainment dataset to the fire perimeter. Here, red triangles are retainment features, and the fire perimeter is the dark grey area right of center:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5cadde-0069-46b1-8a6d-6fb354804dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.retainments(\n",
    "    retainments_path, \n",
    "    perimeter=perimeter, \n",
    "    title='Retainments + Fire Perimeter'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3ebc9-60cd-46f3-b409-93665d6dfec4",
   "metadata": {},
   "source": [
    "As we can see, the Point dataset covers a much larger area than the fire perimeter. Converting the full retainment dataset to a raster would require a lot of unnecessary memory, so we'll use the `bounds` option to only rasterize the dataset within the buffered fire perimeter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36897e9-c201-4553-9ba2-ea1c1c2015c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retainments = Raster.from_points(retainments_path, bounds=perimeter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6417404a-c1c2-4fa7-b361-a28093e11b75",
   "metadata": {},
   "source": [
    "## Reprojection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8213feb6-22bf-45bf-b9c5-bf2cdb7ce8f1",
   "metadata": {},
   "source": [
    "Our next task is to project the rasters into the same CRS. As we saw in the [table above](#Inputs), they currently use 5 different CRSs. We'll select the CRS of the DEM dataset as our target CRS. This is because high-fidelity DEM data is essential for determining flow pathways, and using the DEM CRS will keep this dataset in its original state.\n",
    "\n",
    "You can reproject a `Raster` object using its built-in `reproject` method. In addition to matching CRSs, we also want our datasets to have the same alignment and resolution, and the `reproject` command can do this as well. Here, we'll use the `template` option to reproject each dataset to match the CRS, resolution, and alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6de6a8-1731-4aa5-9559-672ad6f127c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rasters = [perimeter, dem, dnbr, kf, evt, retainments]\n",
    "for raster in rasters:\n",
    "    raster.reproject(template=dem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd94f6d-ccb3-492a-85cf-e0b6c395069c",
   "metadata": {},
   "source": [
    "Inspecting the rasters, we can see they now all have the same CRS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d900b-9696-4a1a-8c6f-d9b831fdf4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for raster in rasters:\n",
    "    print(raster.crs.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a8973-40a2-4682-bf46-2e804757f96f",
   "metadata": {},
   "source": [
    "This tutorial only uses the `template` option for the `reproject` command, but there are also other options, such as the choice of resampling algorithm. As a reminder, you can learn about commands in greater detail in the [User Guide](https://ghsc.code-pages.usgs.gov/lhp/pfdf/guide/index.html) and [API](https://ghsc.code-pages.usgs.gov/lhp/pfdf/api/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e5d897-649d-4deb-b10e-fbfe6af6048b",
   "metadata": {},
   "source": [
    "## Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698a2859-187d-4966-a16d-525bed357d0b",
   "metadata": {},
   "source": [
    "Our next task is to clip the rasters to the same bounding box. The current bounding boxes are similar, but can differ slightly due to the reprojection. You can clip a `Raster` object to a specific bounding box using its built-in `clip` method. Here, we'll use the buffered fire perimeter as the target bounding box, since the perimeter defines the domain of the hazard assessment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075b778-ae1b-43a3-bad0-fd5e438188fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for raster in rasters:\n",
    "    raster.clip(bounds=perimeter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754c596e-d622-4cc9-81cb-fc160c444d6a",
   "metadata": {},
   "source": [
    "Inspecting the rasters, we can see they now all have the same bounding box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50c8c5-427a-428f-8d83-4ace823ae130",
   "metadata": {},
   "outputs": [],
   "source": [
    "for raster in rasters:\n",
    "    print(raster.bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb4ac70-b7a1-486b-b6f8-ac1362accee9",
   "metadata": {},
   "source": [
    "## Data Ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2177c-2d78-4754-a8ee-71897045001f",
   "metadata": {},
   "source": [
    "It's often useful to constrain a dataset to a valid data range, and you can use a `Raster` object's `set_range` command to do so. We'll do this for the dNBR and KF-factor datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc55e70-2841-4497-be53-5f6c05064fa9",
   "metadata": {},
   "source": [
    "### dNBR\n",
    "There is no \"correct\" range for dNBR values, but -1000 to 1000 is a reasonable interval. Inspecting our dNBR data, we can see it includes some values outside of this interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91582662-d10b-41dc-9e5a-209a07a17ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dnbr.values[dnbr.data_mask]\n",
    "print(min(data))\n",
    "print(max(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db9c93b-3a2c-44bb-9e12-08db03f3a63e",
   "metadata": {},
   "source": [
    "Here, we'll use the `set_range` command to constrain the dNBR raster to our desired interval. In this base syntax, data values outside this range will be set to the value of the nearest bound. (NoData pixels are not affected):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff47a904-ddbf-45cb-924b-762720bb6d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnbr.set_range(min=-1000, max=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05728887-7bd0-4eb7-bedf-40d05c7b596f",
   "metadata": {},
   "source": [
    "Inspecting the dataset again, we can see the data values are now within our desired interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a681748-24d8-4c56-8060-1e701983dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dnbr.values[dnbr.data_mask]\n",
    "print(min(data))\n",
    "print(max(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85092e86-b8cb-4302-8e8c-55531d0b8e84",
   "metadata": {},
   "source": [
    "### KF-factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788607e9-3c4a-476f-ba8c-838a8a8db64f",
   "metadata": {},
   "source": [
    "We'll also constrain the KF-factor raster to positive values, as negative KF-factors are unphysical. The STATSGO dataset sometimes uses -1 values to mark large water bodies with missing data, but -1 is not the NoData value, so these -1 values appear as unphysical data values that should be removed from the dataset.\n",
    "\n",
    "We'll use the `set_range` command to ensure the dataset does not include any such values. Here, we'll use the `fill` option, which will replace out-of-bounds data values with NoData, rather than setting them to the nearest bound. We'll also use the `exclude_bounds` option, which indicates that the bound at 0 is not included in the valid data range, so should also be converted to NoData:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd734666-632d-45b9-b50d-85d36d25c253",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf.set_range(min=0, fill=True, exclude_bounds=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e368b-93cb-4626-ac7e-cad9e3dbe935",
   "metadata": {},
   "source": [
    "Inspecting the raster, we can see it does not contain unphysical data values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b97bb-99d1-42d3-8ebc-ffd3f88ebf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = kf.values[kf.data_mask]\n",
    "print(min(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb40ad-07a0-4ffb-a404-a172b3f8321a",
   "metadata": {},
   "source": [
    "## Estimate Severity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a1e1a1-baa1-41b7-91f8-ab662a664c02",
   "metadata": {},
   "source": [
    "Many hazard assessments will require a [BARC4-like](https://burnseverity.cr.usgs.gov/baer/faqs) soil burn severity (SBS) dataset. In brief, a BARC4 dataset classifies burned areas as follows:\n",
    "\n",
    "| Value | Description |\n",
    "| ----- | ----------- |\n",
    "| 1 | Unburned |\n",
    "| 2 | Low burn severity |\n",
    "| 3 | Moderate burn severity |\n",
    "| 4 | High burn severity |\n",
    "\n",
    "This dataset should be sourced from field-validated data when possible, but sometimes such datasets are not available. For example, SBS is often not available for preliminary assessments conducted before a fire is completely contained. When this is the case, you can estimate a BARC4-like SBS from a dNBR or other dataset using the `severity.estimate` command. Let's do that here. We'll start by importing pfdf's `severity` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1db22c7-ef11-419e-9841-9c69a5e881c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfdf import severity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce5534c-094f-485d-be20-9f5e3336ae1f",
   "metadata": {},
   "source": [
    "Next we'll use the `estimate` command to estimate a BARC4 from the dNBR. This command allows you to specify the thresholds between unburned and low, low and moderate, and moderate and high severity. Here, we'll use thresholds of 125, 250, and 500 to classify the dNBR dataset into BARC4 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7d2723-19ca-4a75-ac43-1cd73ce28cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "barc4 = severity.estimate(dnbr, thresholds=[125,250,500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21465785-9633-44d8-840a-60a4205331ff",
   "metadata": {},
   "source": [
    "Plotting the new dataset, we can see it has classified the dNBR into 4 groups, as per our thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d7593-e19c-4a01-9e50-4d89e5247776",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.raster(\n",
    "    barc4, \n",
    "    cmap='OrRd', \n",
    "    title='Estimated Burn Severity', \n",
    "    clabel='BARC4 Code', \n",
    "    show_basemap=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbacff7-1523-4912-82b3-cd34913a5718",
   "metadata": {},
   "source": [
    "## Terrain Masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25ad0e-8d4b-46d1-a0a2-4d77776491bb",
   "metadata": {},
   "source": [
    "Our final task is to build terrain masks from the EVT dataset. Specifically, we want to locate water bodies and human development. Debris flows are unlikely to initiate in water bodies, so it will be useful to exclude them from the hazard assessment. Similarly, human development can alter debris-flow behavior in ways outside the scope of the assessment models, so we'll locate these areas from the assessment as well. The EVT dataset uses different integer codes to classify terrain. The code for water bodies is 7292, and human development is marked by the integers from 7296 to 7300. \n",
    "\n",
    "You can locate specific values in a `Raster` object using the `find` method. This method returns a boolean raster mask that locates the queried values in the initial raster. True values indicate pixels that match one of the queried values, and all other pixels are marked as False. Let's use the `find` command to build our water and development masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de46ed-8619-48f1-95f0-2e9a01bd629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iswater = evt.find(7292)\n",
    "isdeveloped = evt.find([7296, 7297, 7298, 7299, 7300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c78156-0df0-4609-a679-9806891fb617",
   "metadata": {},
   "source": [
    "Plotting the water mask, we can see it marks pixels in the Morris and San Gabriel reservoirs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f4440-5726-4802-98ac-e2faa3e484cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.mask(iswater, title='Water Mask', legend='Water Mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d0c39c-ca50-4252-9754-6e0dc82a21a7",
   "metadata": {},
   "source": [
    "And the development mask marks roadways and greater Los Angeles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c113d2-b6e0-420e-a23c-df7102bbe21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.mask(isdeveloped, title='Development Mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a94485-b87a-4d8f-897f-62d4cf790f02",
   "metadata": {},
   "source": [
    "## Save Preprocessed Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c92a39-bef3-4ca9-bc1b-89d5c9a8577b",
   "metadata": {},
   "source": [
    "We'll now save our preprocessed rasters so we can use them in later tutorials. We'll start by creating a folder for the preprocessed datasets to disambiguate them from the raw datasets in the data folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1693b1-7024-4fd6-8bee-5dfbfe2e854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = Path.cwd() / 'preprocessed'\n",
    "folder.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0597f2-45af-47ff-8515-143a543b55e8",
   "metadata": {},
   "source": [
    "Then, we'll collect and save the preprocessed rasters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdb621d-d321-45d3-ab75-ac3e503ab7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rasters = {\n",
    "    'perimeter': perimeter,\n",
    "    'dem': dem,\n",
    "    'dnbr': dnbr,\n",
    "    'barc4': barc4,\n",
    "    'kf': kf,\n",
    "    'retainments': retainments,\n",
    "    'evt': evt,\n",
    "    'iswater': iswater,\n",
    "    'isdeveloped': isdeveloped,\n",
    "}\n",
    "for name, raster in rasters.items():\n",
    "    raster.save(f'preprocessed/{name}.tif', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc290ce-68ad-4e5a-abe2-3ad15ad84b3c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this tutorial, we've seen how to preprocess datasets for an assessment. We\n",
    "\n",
    "* Rasterized vector datasets,\n",
    "* Reprojected to the same CRS,\n",
    "* Clipped to the same bounding box,\n",
    "* Constrained data ranges,\n",
    "* Estimated severity, and\n",
    "* Located terrain masks.\n",
    "\n",
    "The following section provides a quick-reference script for implementing these steps\n",
    "In the [next tutorial](05_Hazard_Assessment.ipynb), we'll see how to leverage these datasets to implement a hazard assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2970923-6b9d-4655-a45b-dfa808be5068",
   "metadata": {},
   "source": [
    "## Quick Reference\n",
    "This quick reference script collects the commands explored in the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4715255-85f5-4b5b-916e-bab91b0c62f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets this notebook for the script\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0386e65-dc90-4735-b5c7-762d5261a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from pfdf.raster import Raster\n",
    "from pfdf import severity\n",
    "\n",
    "# Load datasets\n",
    "perimeter = Raster.from_file('data/buffered-perimeter.tif')\n",
    "dem = Raster.from_file('data/dem.tif')\n",
    "dnbr = Raster.from_file('data/dnbr.tif')\n",
    "kf = Raster.from_file('data/kf.tif')\n",
    "evt = Raster.from_file('data/evt.tif')\n",
    "\n",
    "# Rasterize debris retainments\n",
    "retainments = Raster.from_points('data/la-county-retainments.gdb', bounds=perimeter)\n",
    "\n",
    "# Reproject rasters to match the DEM projection\n",
    "rasters = [perimeter, dem, dnbr, kf, evt, retainments]\n",
    "for raster in rasters:\n",
    "    raster.reproject(template=dem)\n",
    "\n",
    "# Clip rasters to the bounds of the buffered perimeter\n",
    "for raster in rasters:\n",
    "    raster.clip(bounds=perimeter)\n",
    "\n",
    "# Constrain data ranges\n",
    "dnbr.set_range(min=-1000, max=1000)\n",
    "kf.set_range(min=0, fill=True, exclude_bounds=True)\n",
    "\n",
    "# Estimate severity\n",
    "barc4 = severity.estimate(dnbr, thresholds=[125,250,500])\n",
    "\n",
    "# Build terrain masks\n",
    "iswater = evt.find(7292)\n",
    "isdeveloped = evt.find([7296, 7297, 7298, 7299, 7300])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}