{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f874125-3241-4421-91d6-61300d95069b",
   "metadata": {},
   "source": [
    "# Data Download Tutorial\n",
    "This tutorial shows how to use the ``data`` subpackage to download hazard assessment datasets. Or [skip to the end](#Quick-Reference) for a quick reference script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5475a783-3291-4ada-8b4f-6d05dc742ca3",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f1a3d-0da6-4e52-8b09-811be2c6b2ea",
   "metadata": {},
   "source": [
    "Hazard assessments require a variety of datasets. These include datasets used to delineate a stream segment network (such as fire perimeters and digital elevation models), and datasets used to implement hazard models (such as dNBRs and soil characteristics). Obtaining these datasets can represent a significant effort, requiring navigation of multiple data providers, each with their own systems and frameworks for distributing data.\n",
    "\n",
    "As such, pfdf provides the ``data`` subpackage to help alleviate the difficulty of acquiring assessment datasets. This package provides routines to download various common datasets over the internet. These include:\n",
    "\n",
    "* [USGS digital elevation models (DEMs)](https://www.usgs.gov/3d-elevation-program/about-3dep-products-services),\n",
    "* [NOAA Atlas 14](https://hdsc.nws.noaa.gov/pfds/) rainfall statistics,\n",
    "* [STATSGO](https://www.sciencebase.gov/catalog/item/675083c6d34ea60e894354ad) KF-factors and soil thicknesses,\n",
    "* [LANDFIRE](https://www.landfire.gov/) existing vegetation types (EVTs),\n",
    "* Debris retainment feature locations, and\n",
    "* [Hydrologic unit (HU)](https://www.usgs.gov/national-hydrography/watershed-boundary-dataset) boundaries and datasets.\n",
    "\n",
    "In this tutorial we will use the `data` subpackage to acquire these datasets for the [2016 San Gabriel Fire Complex](https://en.wikipedia.org/wiki/San_Gabriel_Complex_Fire) in California. Note that we will not be downloading the fire perimeter, burn severity, or dNBR datasets in this tutorial, as these datasets are typically provided by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a184099-1cee-4a05-ab74-34ad7360d56b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Important!\n",
    "You are NOT required to use these datasets for your own assessments. The ``data`` subpackage is merely intended to facilitate working with certain commonly used datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09058259-88a5-4931-9f0b-d480adc6e856",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Suggesting Datasets\n",
    "We welcome community suggestions of datasets to include in the ``data`` package. To suggest a new dataset, please contact the developers or the Landslides Hazards Program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f4e68-c543-4a16-ab4e-8200a03d382e",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a27de3e-979d-40b1-b5c6-00fd046a7c6a",
   "metadata": {},
   "source": [
    "### Raster Tutorial\n",
    "This tutorial assumes you are familiar with ``Raster`` objects and their metadata. Please read the [Raster Intro Tutorial](02_Raster_Intro.ipynb) if you are not familiar with these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c330e-ac02-40af-8073-22b0f368b07b",
   "metadata": {},
   "source": [
    "### Install pfdf\n",
    "To run this tutorial, you must have installed [pfdf 3+ with tutorial resources](https://ghsc.code-pages.usgs.gov/lhp/pfdf/resources/installation.html#tutorials) in your Jupyter kernel. The following line checks this is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cc1cd2-c8e0-4eee-8816-04895497273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import check_installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f01bd3-7774-4bf0-b41e-83a8dca7b227",
   "metadata": {},
   "source": [
    "### Clean workspace\n",
    "Next, we'll use the `remove_downloads` tool to remove any previously downloaded datasets from the tutorial workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd60c9b-5b7c-498c-b1ab-3b9b9af2a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import workspace\n",
    "workspace.remove_downloads()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828bf498-4da0-4bb4-9a14-39034cb37370",
   "metadata": {},
   "source": [
    "### Imports\n",
    "We'll next import the ``data`` subpackage from pfdf. We'll also import the `Raster` class, which we'll use to help manage downloaded datasets. Finally, we'll import some tools to help run the tutorial. The tools include the `plot` module, which we'll use to visualize downloaded datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f19de-5375-4c26-a0df-55017fa3a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfdf import data\n",
    "from pfdf.raster import Raster\n",
    "from tools import print_path, print_contents, plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033b01b5-b1ee-4ec1-be3b-73fca199087e",
   "metadata": {},
   "source": [
    "## `data` Package Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af72919-07b0-426c-ad09-e7083476c608",
   "metadata": {},
   "source": [
    "The ``data`` package contains a variety of subpackages, which are typically organized around data providers. Currently, these subpackages include:\n",
    "\n",
    "* ``landfire``: [LANDFIRE](https://landfire.gov/) data products, most notably EVTs\n",
    "* ``noaa``: [NOAA](https://www.noaa.gov/) data products, including [NOAA Atlas 14](https://hdsc.nws.noaa.gov/pfds/)\n",
    "* ``retainments``: Locations of debris retainment features,\n",
    "* ``usgs``: Products distributed by the USGS, including [DEMs](https://www.usgs.gov/3d-elevation-program/about-3dep-products-services), [HUs](https://www.usgs.gov/national-hydrography/watershed-boundary-dataset), and [STATSGO](https://www.sciencebase.gov/catalog/item/675083c6d34ea60e894354ad) soil data\n",
    "\n",
    "The data provider namespaces are organized into further subpackages and modules. Individual modules are generally associated with a specific data product or API distributed by a provider. After importing a dataset's module, you can acquire the dataset using a ``download`` or ``read`` command from the module. A ``download`` command will download one or more files to the local filesystem, whereas a ``read`` command will load a raster dataset as a ``Raster`` object. The following sections will demonstrate the ``download`` and ``read`` commands for various datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9207bbd-a5d5-426d-87fe-8b9c82392e70",
   "metadata": {},
   "source": [
    "## Fire perimeter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114312fe-cb75-4189-a5cd-29e08480c09a",
   "metadata": {},
   "source": [
    "Most hazard assessments begin with a fire perimeter, which is typically provided by the user. The fire perimeter can be useful when downloading data, as you can use it to define a bounding box for your area of interest. You can use this to limit downloaded data to areas within the bounding box, thereby reducing download times. When using a fire perimeter as a bounding box, it's common to first buffer the fire perimeter by a few kilometers. This way, the assessment can include hazardous area downstream of the burn area.\n",
    "\n",
    "The tutorial data includes the fire perimeter for the 2016 San Gabriel Fire Complex as a GeoJSON Polygon file. Let's load that fire perimeter, and buffer it by 3 kilometers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf7fea-0b8c-4b5f-931f-5b500ef80720",
   "metadata": {},
   "outputs": [],
   "source": [
    "perimeter = Raster.from_polygons(\"data/perimeter.geojson\")\n",
    "perimeter.buffer(3, \"kilometers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd4789e-e333-4e24-96ae-1257de14778b",
   "metadata": {},
   "source": [
    "Plotting the perimeter raster, we find it consists of two main burn areas (the Fish and Reservoir fires), with a 3 kilometer buffer along the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dcb72b-2053-408d-8e91-cb9a9649b3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This function may take a bit to download the area basemap\n",
    "plot.mask(perimeter, title='Buffered Fire Perimeter', legend='Burn Area')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3172c1d2-4a7b-436f-bd94-ffe4ce32814b",
   "metadata": {},
   "source": [
    "We'll also save this buffered perimeter for use in later tutorials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3e629-6bc8-4bc9-b2a3-96dd74840a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = perimeter.save('data/buffered-perimeter.tif', overwrite=True)\n",
    "print_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2c85d2-5359-4ed6-b904-a1bbcd752fa7",
   "metadata": {},
   "source": [
    "## DEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46966a33-815f-415d-8c17-22e4ab65f095",
   "metadata": {},
   "source": [
    "Next we'll download a digital elevation model (DEM) dataset for the buffered perimeter. The DEM is often used to (1) design a stream segment network and (2) assess topography characteristics for hazard models. Here, we'll use the USGS's 1/3 arc-second DEM, which has a nominal 10 meter resolution.\n",
    "\n",
    "We can acquire DEM data using the ``data.usgs.tnm.dem`` module. Breaking down this namespace:\n",
    "\n",
    "* ``usgs`` collects products distributed by the USGS,\n",
    "* ``tnm`` collects products from The National Map, and\n",
    "* ``dem`` is the module for DEMs from TNM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffd6599-5c24-4b4d-9007-44307ce6be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfdf.data.usgs import tnm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bafc18b-23e2-4ba5-a0ae-1a55c1c1406c",
   "metadata": {},
   "source": [
    "DEMs are raster datasets, so we'll use the ``read`` command to load the DEM as a ``Raster`` object. We'll also pass the buffered fire perimeter as the ``bounds`` input. This will cause the command to only return data from within the buffered perimeter's bounding box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e7bdb-fdc9-4b9f-8351-ffc71ebe9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem = tnm.dem.read(perimeter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957498e8-1f36-4298-b8c3-a1ff2f1cb9b3",
   "metadata": {},
   "source": [
    "Plotting the DEM, we observe the topography for the assessment area with the San Gabriel mountains to the north, and part of greater Los Angeles in the plain to the south."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8935ab-befd-446e-8b1c-d3a31ed51ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.raster(dem, cmap='terrain', title='Digital Elevation Model (DEM)', clabel='Elevation (meters)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d35ca8-ede3-41c6-97a2-374f2e7b3499",
   "metadata": {},
   "source": [
    "We'll also save the DEM for use in later tutorials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088da3c7-88e6-49c1-83a2-2aef2c89e329",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = dem.save('data/dem.tif', overwrite=True)\n",
    "print_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a0db1-5851-4766-b736-d03ca496ca46",
   "metadata": {},
   "source": [
    "## STATSGO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ea7fa-14f3-435c-bd0a-3fcfcff5620d",
   "metadata": {},
   "source": [
    "Next, we'll use the `data.usgs.statsgo` module to download soil characteristic data from the STATSGO archive within the buffered perimeter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a945ae-e168-4766-ae2d-da32d84d2522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfdf.data.usgs import statsgo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1125f2db-524a-4113-9168-894d6b66c27f",
   "metadata": {},
   "source": [
    "Specifically, we'll download soil KF-factors (KFFACT), which we'll use to run the M1 likelihood model in the [Hazard Assessment Tutorial](05_Hazard_Assessment.ipynb). We'll use the `read` command to load the dataset as a `Raster` object. Once again, we'll pass the buffered fire perimeter as the `bounds` input to only load data within the buffered perimeter's bounding box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a0dc86-5412-4712-b8f4-734c6cc7296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = statsgo.read('KFFACT', bounds=perimeter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f35dd8-08b8-4e73-87b9-3f2d217122ff",
   "metadata": {},
   "source": [
    "Plotting the KF-factors, we find it consists of values from 0.15 to 0.25 distributed over 4 distinct map units within our assessment domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a59425b-d8b9-4a3e-aa5e-abaf22621d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.raster(kf, cmap='turbo', title='KF-factors', show_basemap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca4f2be-cffc-4b83-98ad-5bc1c16b6a8d",
   "metadata": {},
   "source": [
    "We'll also save the KF-factors for use in later tutorials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580eba5a-2ca4-4474-ab06-b074af1c94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kf.save('data/kf.tif', overwrite=True)\n",
    "print_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e2b032-9755-4bff-81a4-05422e89f67b",
   "metadata": {},
   "source": [
    "## EVT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3755f-b28f-401e-9263-a0346584f482",
   "metadata": {},
   "source": [
    "Next, we'll download an existing vegetation type (EVT) raster from [LANDFIRE](https://www.landfire.gov/). EVTs are often used to locate water bodies, human development, and excluded areas prior to an assessment. Here, we'll use data from LANDFIRE EVT version 2.4.0, which has a nominal 30 meter resolution.\n",
    "\n",
    "We can acquire LANDFIRE data using the `data.landfire` module. Here, we'll use the `read` command to read EVT data into memory as a `Raster` object. The `read` command requires the name of a LANDFIRE data layer as input. The name of the EVT 2.4.0 layer is `240EVT`, and you can find a complete list of LANDFIRE layer names here: [LANDFIRE Layers](https://lfps.usgs.gov/lfps/helpdocs/productstable.html). We'll also pass the buffered fire perimeter as the ``bounds`` input so that the command will only read data from within our assessment domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f0727-8a98-45fa-a6fc-4f84d574c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evt = data.landfire.read('240EVT', bounds=perimeter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f01c91-d914-4629-be1d-ad84477142a0",
   "metadata": {},
   "source": [
    "Plotting the dataset, we can observe a variety of classifications. These include:\n",
    "\n",
    "* A large developed area at the bottom of the map (orange),\n",
    "* Mostly undeveloped areas throughout the San Gabriel mountains (dark blue),\n",
    "* Roadways crossing the San Gabriel mountains (orange), and\n",
    "* Water bodies including the Morris and San Gabriel reservoirs (grey blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a5ca8-3d33-4127-af1d-87db651005d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.raster(evt, cmap='tab20', title='Existing Vegetation Type (EVT)', clabel='Vegetation Classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d19c8bd-5224-4819-acd0-cc9ed8a2f808",
   "metadata": {},
   "source": [
    "We'll also save the EVT for use in later tutorials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac03ca5b-97f8-4456-bcfb-6ae08baa181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = evt.save('data/evt.tif', overwrite=True)\n",
    "print_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8d0769-82d2-42cd-9c36-76eae8594626",
   "metadata": {},
   "source": [
    "## Retainment Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc16142-1bdb-4da3-8c79-dc081b35c76a",
   "metadata": {},
   "source": [
    "Next, we'll download the locations of any debris retainment features in our assessment domain. Retainment features are typically human-constructed features designed to catch and hold excess debris. Not all assessment areas will include such features. But when they do, retainment features can be useful for designing the stream segment network, as debris-flows are unlikely to proceed beyond these points.\n",
    "\n",
    "We can acquire retainment feature locations using `data.retainments`. Our assessment is in Los Angeles County, so we'll specifically use the `la_county` module to do so. Here, we'll use the `download` command to download the retainment dataset [Geodatabase](https://en.wikipedia.org/wiki/Geodatabase_(Esri)). By default, the command will download the geodatabase to the current folder, so we'll use the `parent` option to save the dataset in our data folder instead. Regardless of where we save the file, the command will always return the absolute path to the downloaded geodatabase as output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a993306-c464-403b-8010-b23bd553d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = data.retainments.la_county.download(parent='data')\n",
    "print_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3384e3-0dff-4e41-94a2-556ca6108d3f",
   "metadata": {},
   "source": [
    "Plotting the retainment features, we observe over 100 retainment points throughout the county:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2d975e-3028-4cba-bbaa-975b131f9d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: May take a bit to download the county basemap\n",
    "plot.retainments(path, title='Debris Retainment Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf65387-a19b-44e7-b0c7-4bc6e167b79e",
   "metadata": {},
   "source": [
    "## NOAA Atlas 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebefbe5-6475-44fc-9ad7-03001aac1698",
   "metadata": {},
   "source": [
    "Next, we'll download rainfall data from NOAA Atlas 14 at the center of our assessment domain. This data is often used to select design storms for running debris-flow hazard assessment models. We can acquire NOAA Atlas 14 data using `data.noaa.atlas14`. This module requires a latitude and a longitude at which to query rainfall data, so we'll first reproject the fire perimeter's bounding box to EPSG:4326 (often referred to as WGS84), and then extract the center coordinate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bc9380-9823-41d1-9dd1-17e2e37c3c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = perimeter.bounds.reproject(4326)\n",
    "lon, lat = bounds.center\n",
    "print(f'{lon=}, {lat=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf93662-9a74-46df-92f8-2c57110a19e1",
   "metadata": {},
   "source": [
    "We'll now use the `download` command to download rainfall data at this coordinate as a `csv` file in the current folder. By default, the command will download the dataset to the current folder. We'll use the `parent` option to download the file to our data folder instead. Regardless of where we save the file, the command returns the absolute path to the downloaded file as output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a62306a-d2a5-435d-8ff7-5ca77151c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = data.noaa.atlas14.download(lon=lon, lat=lat, parent='data')\n",
    "print_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31dfb36-e3d0-46a3-ab3a-061e513e9f7b",
   "metadata": {},
   "source": [
    "Inspecting the file name, we find this dataset holds mean rainfall intensities in metric units, as calculated using partial duration series (pds). The command also includes options to download min/max values, rainfall depths, values derived from annual mean series (ams), and values in english units. You can learn about these options in the [API](https://ghsc.code-pages.usgs.gov/lhp/pfdf/api/data/noaa/atlas14.html).\n",
    "\n",
    "Opening the downloaded file, we find the following data table:\n",
    "\n",
    "```\n",
    "PRECIPITATION FREQUENCY ESTIMATES\n",
    "ARI (years):,  1, 2, 5, 10, 25, 50, 100, 200, 500, 1000\n",
    "      5-min:, 61,75,96,114,143,167,195,227,276,319\n",
    "     10-min:, 44,54,69,82,102,120,140,163,198,229\n",
    "     15-min:, 35,43,55,66,82,97,113,131,159,184\n",
    "     30-min:, 24,30,38,46,57,67,78,91,111,128\n",
    "     60-min:, 18,22,28,33,42,49,57,66,80,93\n",
    "       2-hr:, 13,16,21,25,31,37,43,50,60,69\n",
    "       3-hr:, 11,14,18,21,27,31,36,42,51,58\n",
    "       6-hr:,  8,10,13,16,20,23,27,31,37,43\n",
    "      12-hr:,  6,7,9,11,14,16,18,21,25,29\n",
    "      24-hr:,  4,5,6,8,10,11,13,15,18,21\n",
    "      2-day:,  2,3,4,5,7,8,9,10,13,15\n",
    "      3-day:,  2,2,3,4,5,6,7,8,10,12\n",
    "      4-day:,  1,2,3,3,4,5,6,7,8,10\n",
    "      7-day:,  1,1,2,2,3,3,4,5,6,6\n",
    "     10-day:,  1,1,1,2,2,3,3,4,4,5\n",
    "     20-day:,  0,1,1,1,1,2,2,2,2,3\n",
    "     30-day:,  0,0,1,1,1,1,1,2,2,2\n",
    "     45-day:,  0,0,1,1,1,1,1,1,1,2\n",
    "     60-day:,  0,0,0,1,1,1,1,1,1,1\n",
    "```\n",
    "\n",
    "The rows of the table are rainfall durations, and the columns are recurrence intervals of different lengths. The data values are the mean precipitation intensities for rainfall durations over the associated recurrence interval. We can use these values to inform hazard assessment models that require design storms, which is discussed in detail in the hazard assessment tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb96fdf0-e0f7-40de-8ed0-ef40ff003426",
   "metadata": {},
   "source": [
    "## Extra Credit: Hydrologic Units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37179b85-cae4-446f-b03c-7f5c64a561f1",
   "metadata": {},
   "source": [
    "As a bonus exercise, we'll next download a hydrologic unit data bundle for our assessment domain. Hydrologic units (HUs) divide the US into catchment drainages of various sizes. Each HU is identified using a unique integer code (HUC), where longer codes correspond to smaller units. Many assessments don't require HU data, but HU boundaries can be useful for large-scale analyses, as they form a natural unit for subdividing assessments.\n",
    "\n",
    "You can download HU data bundles using the `data.usgs.tnm.nhd` module. Breaking down this namespace:\n",
    "\n",
    "* `usgs` collects products distributed by the USGS,\n",
    "* `tnm` collects products from The National Map, and\n",
    "* `nhd` is the module for the National Hydrologic Dataset, which collects the HUs.\n",
    "\n",
    "We'll specifically use the `nhd.download` command, which requires an HUC4 or HUC8 code as input. Note that you must input HUCs as strings, rather than ints. (This is to support HUCs with leading zeros). Much of the San Gabriel assessment falls in HU 18070106, so we'll download the data bundle for that HU. By default, the command will download its data bundle to the current folder, so we'll use the `parent` option to download the bundle to our data folder instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3105c87-ac04-46bb-a8f3-5fec1e93ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = data.usgs.tnm.nhd.download(huc='18070106', parent='data')\n",
    "print_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ee3be7-524f-4cca-adc4-8ce516e0f3a2",
   "metadata": {},
   "source": [
    "From the output file path, we find the data bundle has been downloaded to a folder named \"huc8-18070106\". Inspecting the folder's contents, we find it contains a variety of Shapefile data layers in an internal `Shape` subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182cb292-79cb-4f1d-8fb6-1b29494c89c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_contents(path/\"Shape\", extension=\".shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e133ae-74d0-4cd0-b491-92bf711f609d",
   "metadata": {},
   "source": [
    "Although the `download` command will only accept HU4 and HU8 codes, it's worth noting that the data bundle includes watershed boundaries for HUs 2 through 16. We will not use these datasets in the tutorials, but we note that HU10 boundaries can often be a good starting point for large-scale analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca908c-11ce-440d-b1ba-cff01e1f5cb1",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef794512-40bb-472e-87c7-c382621714a2",
   "metadata": {},
   "source": [
    "In this tutorial, we've learnen how to use `download` and `read` commands in `pfdf.data` to load various datasets from the internet. These commands can help automate data acquisition for assessments, once initial datasets like the fire perimeter have been acquired. In this tutorial, we've downloaded datasets with a variety of resolutions and spatial projections. In the [next tutorial](04_Preprocessing.ipynb), we'll learn ways to clean and reproject these datasets prior to an assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68508803-df5d-4fb7-9b00-d235864adb41",
   "metadata": {},
   "source": [
    "## Quick Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba31d7fe-50b6-4f2c-8dae-c784f5b148b1",
   "metadata": {},
   "source": [
    "This quick reference script collects the commands explored in the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939d7320-2aad-4285-b08a-fd62c71d3d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets this notebook for the script\n",
    "workspace.remove_downloads()\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db081e-8b05-4cdf-a031-ec045ba5a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfdf import data\n",
    "from pfdf.raster import Raster\n",
    "\n",
    "# Build a buffered burn perimeter\n",
    "perimeter = Raster.from_polygons('data/perimeter.geojson')\n",
    "perimeter.buffer(3, 'kilometers')\n",
    "\n",
    "# Load a DEM into memory\n",
    "dem = data.usgs.tnm.dem.read(bounds=perimeter)\n",
    "\n",
    "# Load STATSGO soil data into memory\n",
    "kf = data.usgs.statsgo.read('KFFACT', bounds=perimeter)\n",
    "\n",
    "# Load a LANDFIRE EVT into memory\n",
    "evt = data.landfire.read('240EVT', bounds=perimeter)\n",
    "\n",
    "# Download retainment features\n",
    "retainments_path = data.retainments.la_county.download(parent='data')\n",
    "\n",
    "# Download NOAA Atlas 14 data\n",
    "bounds = perimeter.bounds.reproject(4326)\n",
    "lon, lat = bounds.center\n",
    "atlas14_path = data.noaa.atlas14.download(lon=lon, lat=lat, parent='data')\n",
    "\n",
    "# Download a hydrologic unit data bundle\n",
    "huc_path = data.usgs.tnm.nhd.download(huc='18070106', parent='data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}