{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae24d46-6195-4ecb-ba02-e92331d9977a",
   "metadata": {},
   "source": [
    "# Hazard Assessment Tutorial\n",
    "This tutorial shows how to use pfdf to implement a hazard assessment. Or [skip to the end](#Quick-Reference) for a quick reference script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d00ee9-69df-442c-bc75-d76c9a994b86",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e526905-a515-41f7-bc1d-81084cbf1fbb",
   "metadata": {},
   "source": [
    "Now that we've acquired and preprocessed our datasets, we're finally ready to implement a hazard assessment. In this tutorial we'll follow a typical USGS hazard assessment. In brief, the assessment will include the following steps:\n",
    "\n",
    "* Characterize the watershed,\n",
    "* Design a stream segment network,\n",
    "* Run hazard models on stream segments, and\n",
    "* Export results to common GIS formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ababe6ec-3ef9-48fa-86d9-b47749ff793a",
   "metadata": {},
   "source": [
    "### Stream Segment Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d86f32e-fb28-4466-b66d-b65870f00ca2",
   "metadata": {},
   "source": [
    "A stream segment network is a collection of flow pathways across a landscape, and these flow paths approximate the local drainage networks. When two stream segments meet, the two segments end and a new segment begins at the confluence point. Stream segments form the basis of USGS-style hazard assessments. In these assessments, hazard models are run on each segment, using information derived from the segment's flow path and catchment basin.\n",
    "\n",
    "In the tutorial, we'll start by delineating an initial stream segment network. We'll design this network to exclude areas that aren't at elevated risk of debris-flow hazards. For example, the initial network will exclude segments that aren't downstream of the burn area. \n",
    "\n",
    "After delineating the initial network, we'll next refine the network, filtering it to a final collection of model-worthy stream segments. These final segments will be selected to meet various physical criteria for elevated debris-flow risk. For example, the final network will not contain stream segments with very large catchment areas, as large catchments tend to exhibit flood-like flows, rather than debris flow-like behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8a4d58-4759-454d-bd17-cf29149f95e8",
   "metadata": {},
   "source": [
    "### Hazard Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fee111-8c84-4cff-9fb9-43d8aefdcc5a",
   "metadata": {},
   "source": [
    "We will use a total of 4 hazard models in this tutorial. These include:\n",
    "\n",
    "* Potential sediment volume,\n",
    "* Debris-flow likelihood,\n",
    "* Combined hazard classification, and\n",
    "* Rainfall thresholds.\n",
    "\n",
    "We'll then estimate potential sediment volumes using the [Gartner et al., 2014](https://doi.org/10.1016/j.enggeo.2014.04.008) emergency model. This model uses terrain and fire severity data to estimate potential sediment volume given a set of design peak 15-minute rainfall intensities. These rainfall intensities are sometimes referred to as _design storms_, and should be selected to reflect potential future rainfall scenarios in the burn area. We also note that pfdf also supports the Gartner 2014 longterm assessment model, but we will not discuss this in the tutorial.\n",
    "\n",
    "Next, we'll estimate debris-flow likelihoods using the M1 model of [Staley et al., 2017](https://doi.org/10.1016/j.geomorph.2016.10.019). This model uses terrain, fire severity, and soil data to estimate debris-flow likelihoods given a set of design rainfall intensities. Here, we'll use the same design storms used for the volume model. The pfdf library also supports the M2, M3, and M4 models, but we will not discuss them in the tutorial.\n",
    "\n",
    "We'll then use a combined hazard classification to estimate the relative hazard of each stream segment and catchment basin. Here, we'll use a modification of the classification scheme presented by [Cannon et al., 2010](https://doi.org/10.1130/B26459.1). This scheme considers both debris-flow likelihood and volume to estimate relative hazard. \n",
    "\n",
    "Finally, we'll apply a rainfall threshold model to determine rainfall levels that are likely to cause debris flow events. Unlike the previous levels, this model estimates rainfall levels as output, rather than using design storms as an input parameter. We'll specifically use the inverted M1 model, which estimates the rainfall levels needed to achieve design probability levels for debris-flow events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a4aff8-9123-41ab-860f-5342fcd314c2",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5452599f-9463-4aa2-bba0-d524421494ab",
   "metadata": {},
   "source": [
    "### Install pfdf\n",
    "To run this tutorial, you must have installed [pfdf 3+ with tutorial resources](https://ghsc.code-pages.usgs.gov/lhp/pfdf/resources/installation.html#tutorials) in your Jupyter kernel. The following line checks this is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe24691-1d9e-4c87-80c4-900306c0f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import check_installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb6daf-3937-4a2a-8cd1-4ea3a65e3806",
   "metadata": {},
   "source": [
    "### Preprocessing Tutorial\n",
    "This tutorial uses datasets prepared in the [Preprocessing Tutorial](04_Preprocessing.ipynb). If you have not run that tutorial, then you should do so now. The following line checks the required datasets have been downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d6f60c-ffef-4a72-972d-b0b151f8d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import workspace\n",
    "workspace.check_preprocessed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50de6a6-623c-47b2-87db-679c2cec5bf6",
   "metadata": {},
   "source": [
    "### Imports\n",
    "We'll need to import many different pfdf components to implement the hazard assessment. We'll defer these imports to the associated sections below to help with organization. For now, we'll just import a few tools used to run the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfcf8f-52b3-4d93-8705-c6dc377e9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import plot, print_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc9f182-0222-4a4a-b368-00678de09a6a",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b2833-6066-4f97-9a6f-83d9c51517bc",
   "metadata": {},
   "source": [
    "Our first step is to load the preprocessed datasets. We'll do this with the `Raster` class, using the `from_file` factory. We'll use the `isbool` option for several datasets so they are correctly loaded as boolean masks, rather than as integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0081d750-e13a-42ac-a8c5-4abf3223ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfdf.raster import Raster\n",
    "\n",
    "perimeter = Raster.from_file('preprocessed/perimeter.tif', isbool=True).values\n",
    "dem = Raster.from_file('preprocessed/dem.tif')\n",
    "dnbr = Raster.from_file('preprocessed/dnbr.tif')\n",
    "kf = Raster.from_file('preprocessed/kf.tif')\n",
    "barc4 = Raster.from_file('preprocessed/barc4.tif')\n",
    "iswater = Raster.from_file('preprocessed/iswater.tif', isbool=True).values\n",
    "isdeveloped = Raster.from_file('preprocessed/isdeveloped.tif', isbool=True)\n",
    "isretainment = Raster.from_file('preprocessed/retainments.tif', isbool=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb0044a-e0fb-4087-a75e-847796ed88b9",
   "metadata": {},
   "source": [
    "## Characterize Watershed\n",
    "Next, we'll characterize the watershed in our area of interest. Specifically, we will locate areas burned at various severities, determine flow directions, and compute various physical properties. This characterization will help us (1) delineate the stream segment network, and (2) quantify terrain and fire severity variables for hazard models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af3f479-c317-4f50-bf56-c0785c82cf36",
   "metadata": {},
   "source": [
    "### Burn Severity Masks\n",
    "We'll begin the watershed characterization by building two burn severity masks. The first mask will locate all pixels that were burned at any level. The second mask will locate pixels that were burned at moderate-or-high severity.\n",
    "\n",
    "You can build severity masks using the `mask` function in the `severity` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f92031d-492b-41db-9acc-aff5bf01ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfdf import severity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9235706-a6ed-40a5-b39b-f6c2ee4295fc",
   "metadata": {},
   "source": [
    "This function takes a BARC4-like raster as input and returns a raster mask for the queried burn severity levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299ba4cc-cabb-45a8-9a3a-003c79e9e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "isburned = severity.mask(barc4, \"burned\")\n",
    "moderate_high = severity.mask(barc4, [\"moderate\", \"high\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4141a-1fdb-4639-9d84-9499865f18da",
   "metadata": {},
   "source": [
    "### Flow Directions\n",
    "Next, we'll use the `watershed` module to analyze the watershed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4fbe8-7056-4832-8c2a-659f2b322504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfdf import watershed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c69d31-3075-44e2-b674-cdc840311f78",
   "metadata": {},
   "source": [
    "We'll start by using a conditioned DEM to compute flow directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf960461-4009-4fd6-a112-d8e1a40585df",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioned = watershed.condition(dem)\n",
    "flow = watershed.flow(conditioned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6317e315-b462-437c-8c10-081ef895b7e7",
   "metadata": {},
   "source": [
    "The output flow direction raster uses the integers from 1 to 8 to mark flow directions. For pixel X, flow is denoted as follows:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "4 & 3 & 2\\\\\n",
    "5 & \\mathrm{X} & 1\\\\\n",
    "6 & 7 & 8\\\\\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "For example, if pixel X flow to the pixel up and left, then the flow number for pixel X will be 3. Plotting our flow direction raster, we can get a rough sense of the flow planes in our watershed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c7b91a-c15c-41a4-a1ac-661fd65113ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.raster(flow, cmap='viridis', title='Flow Directions', clabel='Flow Number')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd8d5c-f054-4948-a137-84ddd17d66ba",
   "metadata": {},
   "source": [
    "We'll then use the flow directions to compute physical properties for the watershed. Specifically, we'll determine each pixel's flow slope, and its vertical relief to the nearest ridge cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1849fc36-52ba-4433-9bb5-f24a0811d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes = watershed.slopes(conditioned, flow)\n",
    "relief = watershed.relief(conditioned, flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07322e3a-3f68-4e75-a342-851f7d1bdcce",
   "metadata": {},
   "source": [
    "### Flow Accumulation\n",
    "Finally, we'll use the `watershed.accumulation` function to compute several types of flow accumulations. By default, the `accumulation` function will count the number of upstream pixels for each point on the raster. However, you can use the `mask` option to only count upstream pixels that meet the mask criterion instead. For example, we'll start by using the retainment feature mask to count the number of retainments above each pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55df0ee1-9317-4654-8392-6aad19e6d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "nretainments = watershed.accumulation(flow, mask=isretainment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef69bda6-7bd5-4096-b9c1-b3d8aeec6aad",
   "metadata": {},
   "source": [
    "We'll then compute the total catchment area, and total burned catchment area for each pixel. Here, we'll use the `times` option to multiply pixel counts by the area of a raster pixel. This way, the area rasters will both have units of square kilometers, rather than pixel counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f8366-107f-421b-86d0-14c7de4d2abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_area = dem.pixel_area(units='kilometers')\n",
    "area = watershed.accumulation(flow, times=pixel_area)\n",
    "burned_area = watershed.accumulation(flow, mask=isburned, times=pixel_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5367b95e-0c49-4149-a3ca-78baec193dd9",
   "metadata": {},
   "source": [
    "## Stream Segment Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ce99c2-229d-4bb0-96db-cd68e53ebb26",
   "metadata": {},
   "source": [
    "We'll next design a stream segment network using the `Segments` class from the `pfdf.segments` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ac21e-c90f-4461-83a3-3f926daaa2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfdf.segments import Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb46e30f-6c75-4abb-9497-7d87ea6091db",
   "metadata": {},
   "source": [
    "### Delineation Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a9cad-7201-4d01-a62a-af743bcf14d6",
   "metadata": {},
   "source": [
    "To create a stream segment network, we'll first require a network delineation mask. This mask is used to exclude non-viable pixels from the network. False pixels will **never** be included in a stream segment. By contrast, a True pixel *may* be included in the network, but there's no guarantee.\n",
    "\n",
    "As a starting point, most masks should exclude pixels with catchments that are too small to generate a debris flow. We'll also exclude catchments that are negligibly burned, as these areas are unlikely to exhibit altered debris-flow hazards. Finally, we'll exclude pixels below debris-flow retainment features, as debris flows are unlikely to proceed beyond these points.\n",
    "\n",
    "We'll start by defining two parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e3fb49-4c11-45d3-940e-6874a3c5175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_area_km2 = 0.025\n",
    "min_burned_area_km2 = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8162ef8b-7cdc-4e6b-8831-fe397d3cacdb",
   "metadata": {},
   "source": [
    "Here, `min_area_km2` defines the minimum catchment area (in square kilometers). Smaller catchments are usually too small to be able to generate a debris-flow. The `min_burned_area_km2` parameter defines the minimum burned catchment area (also square kilometers). Catchments with smaller burned areas are negligibly affected by the fire. We can compare these thresholds to the flow accumulation rasters to help build several criteria masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2473b5-9e51-4fb5-b9a0-2dfba4eb7a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_enough = area.values >= min_area_km2\n",
    "below_burn = burned_area.values >= min_burned_area_km2\n",
    "below_retainment = nretainments.values > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac62c717-a5e2-48c6-9b1d-4e6a72bb0cfc",
   "metadata": {},
   "source": [
    "Here, `large_enough` indicates catchments that are large enough to generate debris flows, and `below_burn` indicates catchments that are sufficiently burned as to have elevated debris-flow risk. Finally, `below_retainment` indicates areas that are below a retainment feature, and therefore are shielded from debris flows.\n",
    "\n",
    "We can use these criteria to build the final delineation mask. First we'll define \"at risk\" areas as anywhere within or downstream of the burn area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a3f1c1-cb9a-44c2-b561-b39ff2ec2dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "at_risk = perimeter | below_burn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71bc42f-9bea-403e-b25f-f939e36bef1f",
   "metadata": {},
   "source": [
    "Then, we'll set the delineation mask to include catchments that are sufficiently large and \"at risk\", but to exclude water bodies and catchments shielded by retainment features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d7500-9365-4073-ab8a-63715eb04563",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = large_enough & at_risk & ~iswater & ~below_retainment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48011968-34e6-45f6-bdc0-2b117751c5a2",
   "metadata": {},
   "source": [
    "Plotting the delineation mask, we find it consists of various stream pathways in and around the burn area. Because this figure has a lower resolution than the actual mask, the plot may look like a series of disjointed pixels. But in reality, most of the pixels connect continuously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4b2089-59c1-4e36-abaa-4cd4c7eb01dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.mask(mask, title='Delineation Mask', spatial=dem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b14080-e264-409d-919b-c11e94a661db",
   "metadata": {},
   "source": [
    "### Delineate Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c9974-2381-45c6-a463-d8d15011d493",
   "metadata": {},
   "source": [
    "We're just about ready to delineate the stream network. Let's define one additional parameter first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d0cd8-f7ad-4080-9e53-ad5fc8fad00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_m = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac16f8a-4f81-4257-8dad-9db8c84daa5e",
   "metadata": {},
   "source": [
    "This parameter establishes a maximum length for segments in the network (in meters). Segments longer than this length will be split into multiple pieces. We can now use the `Segments` constructor to delineate the network. This will return a new `Segments` object that manages our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0310b11-44c0-4f94-9237-df0d0c32df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = Segments(flow, mask, max_length_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe231f1-640c-4ffa-8ec0-2793255bc516",
   "metadata": {},
   "source": [
    "Inspecting the object, we find the stream network consists of 696 stream segments, which are distributed in 41 local drainage networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ccbf0c-e95d-471a-a04f-e75b180438d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2291b9-f1e4-425d-8053-7e5e7d28943b",
   "metadata": {},
   "source": [
    "Plotting the network, we finde the segments are distributed in and around the fire perimeter. Here, blue lines are stream segments, the fire perimeter is in grey, and red triangles indicate retainment features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f83baea-7f5c-40d4-831a-c8aef9d5bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.network(segments, title='Initial Network', perimeter=perimeter, show_retainments=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318bf26e-aaeb-45c1-9630-22d5af2d282a",
   "metadata": {},
   "source": [
    "### Filter Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61f46fc-176e-4839-8c67-0831f59e8c57",
   "metadata": {},
   "source": [
    "Next, we'll refine the network to a collection of model-worthy segments. We'll do this by filtering out segments that don't meet various physical criteria for debris-flow risk. Here, we'll consider:\n",
    "\n",
    "----\n",
    "\n",
    "**Catchment Area**\n",
    "\n",
    "Segments with very large catchments will likely exhibit flood-like behavior, rather than debris-flow like. As such, we'll remove segments with very large catchments.\n",
    "\n",
    "**Burn Ratio**\n",
    "\n",
    "The catchment must be sufficiently burned, or it will be negligibly affected by the fire. Here, we'll remove segments whose catchments are insufficiently burned.\n",
    "\n",
    "**Slope**\n",
    "\n",
    "Debris flows are most common in areas with steep slopes, as shallow slopes can lead to sediment deposition, rather than debris flow. We'll remove segments whose slopes are too shallow.\n",
    "\n",
    "**Confinement Angle**\n",
    "\n",
    "Debris flows are more common in confined areas, as open areas can allow debris deposition, rather than flow. We'll remove segments that are insufficiently confined.\n",
    "\n",
    "**Developed Area**\n",
    "\n",
    "Human development can alter the course and behavior of debris flows, so we'll remove segments that contain large amounts of human development.\n",
    "\n",
    "---\n",
    "\n",
    "With the exception of catchment area, we will only filter segments that are *outside* of the fire perimeter. Segments within the perimeter will be retained, regardless of physical characteristics. We make this choice because removing segments in the perimeter can result in assessment maps that appear incomplete to stakeholders unfamiliar with the assessment process. Ultimately, retaining all segments in the perimeter tends to reduce stakeholder confusion.\n",
    "\n",
    "We'll start by defining several filtering parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e058db75-f363-4acc-b808-bed8b992873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_burn_ratio = 0.25\n",
    "min_slope = 0.12\n",
    "max_area_km2 = 8\n",
    "max_developed_area_km2 = 0.025\n",
    "max_confinement = 174\n",
    "neighborhood = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6783c850-56a3-42ff-9135-d93a60929002",
   "metadata": {},
   "source": [
    "Most of these parameters define a threshold for one of the filtering checks. The final `neighborhood` parameter indicates the pixel radius of the focal window used to compute confinement angles.\n",
    "\n",
    "Next, we'll use the `segments` object to compute the filtering variables for each segment. Note that the `area` and `developed_area` commands return values in square kilometers by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92703833-eee4-4bf5-afe1-2439efc432bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_km2 = segments.area(units='kilometers')\n",
    "burn_ratio = segments.burn_ratio(isburned)\n",
    "slope = segments.slope(slopes)\n",
    "confinement = segments.confinement(dem, neighborhood)\n",
    "developed_area_km2 = segments.developed_area(isdeveloped)\n",
    "in_perimeter = segments.in_perimeter(perimeter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407ee508-2083-4086-be2e-551882f2d2cd",
   "metadata": {},
   "source": [
    "Here, the output variables are 1D numpy arrays with one element per segment in the network. For example, inspecting the `area_km2` array gives us the total catchment area for each segment. (Note that we're only showing values for the first 10 segments here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4ac11-5261-47a3-a498-1a76058cef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(area_km2[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90afd2d1-2dde-44f9-ae1c-720be2549f0e",
   "metadata": {},
   "source": [
    "We'll then compare the variables to the thresholds. The resulting arrays are boolean vectors with one element per segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f14b1-f2fa-4155-be76-cc129f4042c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "floodlike = area_km2 > max_area_km2\n",
    "burned = burn_ratio >= min_burn_ratio\n",
    "steep = slope >= min_slope\n",
    "confined = confinement <= max_confinement\n",
    "undeveloped = developed_area_km2 <= max_developed_area_km2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0873c9ed-89bb-4caf-b6b2-e5f24dd96c6f",
   "metadata": {},
   "source": [
    "For example, we can inspect the `burned` array to determine which segments have sufficiently burned catchments. Here, we find that segment 5 has a sufficiently burned catchment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57fb725-2be9-4f73-961d-6da232f19be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(burned[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c17e08-b5fa-4ba6-9bcf-6339cb1ff143",
   "metadata": {},
   "source": [
    "Finally, we'll use these arrays to filter the network. We'll remove all flood-like segments, and we'll retain any segments that :\n",
    "\n",
    "* Are in the fire perimeter,\n",
    "* Meet physical criteria for debris-flow risk, or\n",
    "* Would cause a [flow discontinuity](https://ghsc.code-pages.usgs.gov/lhp/pfdf/guide/segments/filter.html#flow-continuity) if removed.\n",
    "\n",
    "We use the `continuous` function to implement the final criteria. In its default configuration, it will return a boolean vector indicating the segments that either (A) were indicated should be kept, or (B) should be kept to preserve flow continuity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d3b64-a31f-41b7-a7da-17d46e5a3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "at_risk = burned & steep & confined & undeveloped\n",
    "keep = ~floodlike & (in_perimeter | at_risk)\n",
    "keep = segments.continuous(keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c927df6-c839-44b5-9ae7-bfdc22f93633",
   "metadata": {},
   "source": [
    "Let's visualize the filtering results - segments being retained are in blue, and segments being removed are in red. For the most part, the segments being removed either have very large catchments (and anticipated flood-like behavior), or they are on relatively flat terrain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b067cb5-ece5-4dd9-9d34-22c3c88de77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.network(segments, title='Filtering Results', keep=keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef251a-fcbd-450c-8db3-cae8ecb5e938",
   "metadata": {},
   "source": [
    "We'll filter the network to our preferred segments using the `keep` command. Inspecting the network afterward, we find the network has been reduced to 460 segments in 92 local drainage networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ddedfa-0107-4c23-b27b-ec44e869eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments.keep(keep)\n",
    "print(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce8c632-7cae-408e-ae03-7b6df1af2087",
   "metadata": {},
   "source": [
    "Plotting the network, we find it now only includes of our preferred stream segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699a7f08-20a8-4413-b73c-f5a4fbb32e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.network(segments, title='Final Network')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a465ea94-1c01-4ceb-bcf9-42169b1bd2ff",
   "metadata": {},
   "source": [
    "## Hazard Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3167cba6-f3c2-4c77-a2c5-6a31d5ef5a6d",
   "metadata": {},
   "source": [
    "We've finished designing the stream network, so we're now ready to implement the hazard models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd6c20-221d-48fd-8975-759a2f388646",
   "metadata": {},
   "source": [
    "### Volume\n",
    "\n",
    "We'll start with the potential sediment volume model. As a reminder, we'll be using the emergency assessment model from [Gartner et al., 2014](https://doi.org/10.1016/j.enggeo.2014.04.008). You can implement the models from this paper using the `models.g14` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776eec7d-f9d7-4b73-a16b-260970994b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfdf.models import g14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6111913-c603-47ff-bdb7-c7c3f2798e51",
   "metadata": {},
   "source": [
    "The emergency assessment volume model uses terrain and fire severity data to estimate sediment volume. The model also requires a set of selected peak 15-minute rainfall intensities (mm/hour), often referred to as \"design storms\". We'll start by selecting some design rainfall intensities for our assessment. Looking back at the NOAA Atlas 14 data we downloaded in the data tutorial, we find that - within our area of interest - the peak 15-minute rainfall intensity for a 1 year recurrence interval is 35 mm/hour. We'll run the model for 35 mm/hour and also several adjacent values to capture a range of potential storm scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada7a36e-da7c-4a5d-8e67-5ac06617a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "I15_mm_hr = [16, 24, 35, 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dbb54f-f7ae-408f-9332-5dfa8cb105b3",
   "metadata": {},
   "source": [
    "These values are reasonable for the San Gabriel assessment area, but other assessments will likely need different design storm values. As we saw in the [Data Tutorial](03_Download_Data.ipynb), you can use the `data.noaa.atlas14` to download rainfall recurrence intervals for an area, and there are many other useful rainfall datasets online.\n",
    "\n",
    "The volume model also requires terrain and fire severity data for each stream segment. Specifically, it requires each segment's vertical relief (relief), and the total catchment area burned at moderate-or-high intensity (Bmh_km2). We can use the `segments` object to compute these variables given various watershed characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd1d1f-deb2-4fe6-9358-9df7be928d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bmh_km2 = segments.burned_area(moderate_high)\n",
    "relief = segments.relief(relief)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf169f-e3bf-493a-ad8e-806d263a695a",
   "metadata": {},
   "source": [
    "Here, each variable is a 1D numpy array with one value per segment in the network. We can now use the `g14.emergency` function to run the volume model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108cfb6-ea0f-48b8-877e-fb8f4f239601",
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes, Vmin, Vmax = g14.emergency(I15_mm_hr, Bmh_km2, relief)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8478ac9d-9403-4696-aa86-95103c96c82d",
   "metadata": {},
   "source": [
    "Here, `volumes` holds the central volume estimates, and `Vmin` and `Vmax` are the upper and lower bounds of the 95% confidence interval. The outputs are 2D arrays: each row is a stream segment, and each column holds the results for a design storm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d74da9-f389-476a-af5b-32feb102c177",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(volumes.shape)\n",
    "print((segments.size, len(I15_mm_hr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83119b27-2a4f-4c69-b89b-7b85261ea098",
   "metadata": {},
   "source": [
    "Let's visualize the results for a design storm with a peak 15-minute rainfall intensity of 24 mm/hour (`V[:,1]`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b89111b-5b2c-4dc6-8409-01ea0787259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1 # The index of the design storm we are plotting\n",
    "plot.volumes(segments, volumes[:,k], title='Potential Sediment Volume', I15=I15_mm_hr[k], clabel='Cubic meters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d7d13-e57c-47f3-bb38-ef0c96ac8590",
   "metadata": {},
   "source": [
    "### Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d173e0-9997-458c-9075-03ce01bca8f1",
   "metadata": {},
   "source": [
    "Next, we'll use the M1 model from [Staley et al., 2017](https://doi.org/10.1016/j.geomorph.2016.10.019) to estimate debris-flow likelihoods. You can implement the models from this paper using the `models.s17` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45aa673-dc23-435e-abb6-6be749782acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfdf.models import s17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a77ac9e-77c3-4f8e-9bc3-939da204ae23",
   "metadata": {},
   "source": [
    "As a reminder, this model uses terrain, fire severity, and soil data to estimate debris-flow likelihoods given a set of design storms. The M1 model supports design storms for multiple rainfall durations, but we'll limit ourselves to the same 15-minute rainfall intensities used to run the volume model. This way, the likelihood and volume results will be comparable, which is required to classify combined hazard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108cb8c6-96d7-4a72-8b33-bd5a07d860c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(I15_mm_hr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be282149-0975-49d1-861d-3e3f7b9669ba",
   "metadata": {},
   "source": [
    "The model is calibrated to different parameters for different rainfall durations, so we'll first query the calibration parameters for 15-minute intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a19cb2-5a84-4daa-903a-6f804a1c2018",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, Ct, Cf, Cs = s17.M1.parameters(durations=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb2760-03d3-4d11-88ca-458dfb52a481",
   "metadata": {},
   "source": [
    "These are the model intercept `B`, and coefficients for the terrain `Ct`, fire `Cf`, and soil `Cs` variables. Also, the M1 model expects rainfall *accumulations*, not intensities, so we'll need to use the `intensity` module to convert our design storms to accumulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49675986-3d11-4970-9aaa-18ba59358e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfdf.utils import intensity\n",
    "R15 = intensity.to_accumulation(I15_mm_hr, durations=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4610e093-e6c5-4ca2-912e-fbdbc1d1ead9",
   "metadata": {},
   "source": [
    "Inspecting these values, we cfind they've been divided by 4 to convert from 15-minute intensities to accumulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f77dcce-b21a-424b-b15f-48b76a1d0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(R15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca21da1-50ed-46a9-adfe-c90533960d32",
   "metadata": {},
   "source": [
    "Next, the model requires terrain, fire severity, and soil data. Specifically, these are:\n",
    "\n",
    "* Terrain: The proportion of catchment area with both (1) moderate-or-high burn severity, and (2) a slope angle of at least 23 degrees\n",
    "* Fire: Mean catchment dNBR divided by 1000\n",
    "* Soil: Mean catchment KF-factor\n",
    "\n",
    "We can compute these by calling the `s17.M1.variables` method with various input datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d0568a-16ef-474a-9cca-d20531f22dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "T, F, S = s17.M1.variables(segments, moderate_high, slopes, dnbr, kf, omitnan=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa8275-c5d1-48f3-b16a-12a93593f5a4",
   "metadata": {},
   "source": [
    "Now that we've collected our inputs, we can run the model using the `s17.likelihood` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487fd24a-a86c-43c0-b484-de508afc799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoods = s17.likelihood(R15, B, Ct, T, Cf, F, Cs, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc3a37-9206-489d-9f88-579f2360dea8",
   "metadata": {},
   "source": [
    "Here, the `likelihoods` output is a 2D array. Each row corresponds to a stream segment, and each column holds results for one of the design storms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6db330-a395-423e-99b9-8b99a9a4d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(likelihoods.shape)\n",
    "print((segments.size, len(R15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2abd7e-5a97-4f63-92a2-b1057b4a4632",
   "metadata": {},
   "source": [
    "Let's visualize the results for a design storm with a peak 15-minute rainfall intensity of 6 mm/hour (`likelihoods[:,1]`):. This is equivalent to a design storm with a peak 15-minute *intensity* of 24 mm/hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a9928-3edc-43b4-86f5-a1d11dcdd1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1  # The index of the design storm we are plotting\n",
    "plot.likelihood(segments, likelihoods[:,k], title='Debris Flow Likelihood', I15=I15_mm_hr[k], clabel='Likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcecbdca-c679-4679-9690-7d12c3d44f9a",
   "metadata": {},
   "source": [
    "### Combined Hazard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc32ac-baca-4cb0-bce9-774bda6cd95c",
   "metadata": {},
   "source": [
    "Now that we've estimated likelihoods and sediment volumes, we can classify each segment's combined relative hazard. We'll use a modified version of the classification presented by [Cannon et al., 2010](https://doi.org/10.1130/B26459.1). In brief, this model considers a segment's likelihood and volume results and assigns the segment a score of 1, 2, or 3, which indicate the following:\n",
    "\n",
    "| Value | Relative Hazard |\n",
    "| ----- | --------------- |\n",
    "| 1 | Low hazard |\n",
    "| 2 | Moderate hazard |\n",
    "| 3 | High hazard |\n",
    "\n",
    "You can implement this model using the `models.c10` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9342f1-e52b-4929-bfa2-55c2cf5597c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfdf.models import c10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87297a74-5e31-4f2e-95ef-1c8b25c9c36a",
   "metadata": {},
   "source": [
    "The original model uses 3 likelihood thresholds, but we'll use the `p_thresholds` variable to use 4 thresholds instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33cc520-6bad-4271-946f-7ea4c30bbaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_thresholds = [0.2, 0.4, 0.6, 0.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207fa3c9-8e29-4b76-8a9e-9639384b4a18",
   "metadata": {},
   "source": [
    "We can then classify combined hazards using the `hazard` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aca5ad-5c44-4f28-bd9c-00d66dd04b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "hazards = c10.hazard(likelihoods, volumes, p_thresholds=p_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cea6869-ab4c-4745-813f-479946f74cb2",
   "metadata": {},
   "source": [
    "The output is a 2D array with one row per segments, and one column per design storm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384bdc86-0d8b-4e28-8604-d35a3c2acbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hazards.shape)\n",
    "print((segments.size, len(R15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0636cc06-dbf8-48ef-b8ae-2205412ea5e4",
   "metadata": {},
   "source": [
    "Let's inspect the results for a design storm with a peak 15-minute rainfall intensity of 24 mm/hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a093611d-0fa3-4803-90bd-143bbdc9a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1  # The index of the design storm we are plotting\n",
    "plot.hazard(segments, hazards[:,k], title='Combined Hazard Class', I15=I15_mm_hr[k], clabel='Hazard Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5161306-9272-4858-9bc1-50d62ec13f2c",
   "metadata": {},
   "source": [
    "### Rainfall Thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d49ec-cc42-43a1-9430-9cb6a3d66ec5",
   "metadata": {},
   "source": [
    "Finally, we'll run a rainfall threshold model - specifically, the inverted M1 likelihood model. Unlike the volume and likelihood models, the rainfall threshold model does not use design storms as an input, so we're not tied to the design storms used by the volume model. Instead, we'll run the rainfall threshold model for 15, 30, and 60 minute rainfall durations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461baa8f-1886-4b52-88eb-2f8297256b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = [15, 30, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b80c5-f72c-4350-8b6a-71294a7dcd02",
   "metadata": {},
   "source": [
    "We'll also define the design probability levels used to estimate rainfall thresholds. Here, we'll estimate thresholds for debris-flow events at the 50% and 75% probability levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15b9af4-3d29-4599-8517-c2414d5ad72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = [0.5, 0.75]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11a54cd8-d191-4816-ab56-b11d3101c360",
   "metadata": {},
   "source": [
    "We already calculated the terrain, fire, and soil variables when we ran the likelihood model, so we'll use them again here. However, we previously only acquired calibration parameters for 15-minute rainfall durations. To run the threshold model for multiple rainfall durations, we'll need to query the calibration parameters for the full set of durations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efdd46d-b88e-4892-89fb-5250678aed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, Ct, Cf, Cs = s17.M1.parameters(durations=durations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6bfb36-a383-4410-abea-d9ff2ab67fc3",
   "metadata": {},
   "source": [
    "Then, we can run the model using the `accumulation` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8fc4e1-05e9-4f0d-b25c-171a865ea5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulations = s17.accumulation(probabilities, B, Ct, T, Cf, F, Cs, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e9a674-b20c-4658-887b-0bc3ca92035d",
   "metadata": {},
   "source": [
    "The output is a 3D numpy array with one row per stream segment, one column per design probability, and one page per rainfall duration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaac114-21d1-4f0b-8a2a-55446b691fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accumulations.shape)\n",
    "print((segments.size, len(probabilities), len(durations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81447673-0f4b-4f81-943e-6ff8324f22c6",
   "metadata": {},
   "source": [
    "The M1 model returns rainfall *accumulations* when inverted, but many users prefer working with rainfall intensities instead. Before continuing, we'll use the `intensity` module to convert the accumulations to intensities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91faf8-414f-41e5-ac90-490e1b6a8c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities = intensity.from_accumulation(accumulations, durations=durations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e991bc-c87a-406d-893e-f295beb11d6b",
   "metadata": {},
   "source": [
    "Inspecting the first element in the two arrays, we find it has been converted from a 15-minute accumulation to a 15-minute intensity. Essentially, the value has been multiplied by 4 to convert mm/15-minutes to mm/hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd7040-b779-4a6b-8adf-ff9afcfb0529",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accumulations[0,0,0])\n",
    "print(intensities[0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659813e-0284-49ef-b5d4-d85e91406708",
   "metadata": {},
   "source": [
    "Let's inspect the 30-minute thresholds for debris-flow events at the 50% probability level (`accumulations[:,0,1]`). Here, lower values are more hazardous, as less rainfall is required to trigger debris-flow events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c020b6b-fd84-42d1-ad35-c6b2417e4305",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0  # The index of the probability level\n",
    "k = 1  # The index of the design storm\n",
    "plot.thresholds(\n",
    "    segments, \n",
    "    intensities[:,p,k],\n",
    "    title='Rainfall Thresholds (mm/hour)',\n",
    "    I15=I15_mm_hr[k], \n",
    "    p=probabilities[p],\n",
    "    clabel='Rainfall Intensities (mm/hour)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c2b0b-6bd3-4efb-916e-2a423f960f52",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3a4753-8ec4-4919-9e15-11fbdc6540f2",
   "metadata": {},
   "source": [
    "We've illustrated much of this tutorial using [matplotlib](https://matplotlib.org/stable/users/index.html), but most users will want to export assessment results to a standard GIS file format. For example, as a Shapefile, Geodatabase, or GeoJSON. In this final section, we'll examine how to export results for:\n",
    "\n",
    "* Stream segments (LineStrings),\n",
    "* Basins (Polygons), and\n",
    "* Outlets (Points)\n",
    "\n",
    "We'll do this using the `segments.save` method, and we'll include various results from our assessment, including hazard model results, model inputs, and earth-system variables for the segments. We'll start by creating a folder to hold our exported files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c05e17-b5c7-494d-b21e-98e2312a432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "exports = Path.cwd() / 'exports'\n",
    "exports.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7839bc7-2a3c-4b50-85bc-73c0062c490c",
   "metadata": {},
   "source": [
    "### Important!\n",
    "The variables we export in this tutorial are completely arbitrary. You are not required to export these variables in your own code, and you can export many other variables as well. Here, we neglected many variables for the sake of brevity. The field names are also arbitrary - you can use any names you like in your own code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69804e5-4b87-4775-bd6f-22106f625970",
   "metadata": {},
   "source": [
    "### Tip: File Formats\n",
    "This tutorial saves results as shapefiles, but you can use [most other GIS formats](https://ghsc.code-pages.usgs.gov/lhp/pfdf/guide/utils/driver.html#vector-formats) instead. For example, to save results as GeoJSON, change the `.shp` file extensions to `.geojson`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ce6e8-70a3-48c8-bd01-d8c8ad87a129",
   "metadata": {},
   "source": [
    "### Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26756bd-2c8b-4e64-a3bc-d654206a75fe",
   "metadata": {},
   "source": [
    "We'll start by exporting the stream segments as a collection of LineString features. In addition to the segment geometries, we'd like to include some of our assessment results in the exported shapefile. We can do this by building a properties `dict` for the export. In general, the keys of the `dict` should be strings and will be the names of the data fields in the exported file. Each key value should be a 1D numpy array with one element per stream segment.\n",
    "\n",
    "For the tutorial, let's export some of the watershed characteristics we used to filter the network, as well as hazard modeling inputs and results. Specifically, let's export:\n",
    "\n",
    "| Type | Field | Description |\n",
    "| ---- | ----- | ----------- |\n",
    "| **Results** | | |\n",
    "| | H_24 | Hazard score for a design storm of 24 mm/hour |\n",
    "| | H_35 | Hazard score for a design storm of 35 mm/hour |\n",
    "| | H_40 | Hazard score for a design storm of 40 mm/hour |\n",
    "| | P_35 | Debris-flow likelihood for a design storm of 35 mm/hour |\n",
    "| | V_35 | Potential sediment volume for a design storm of 35 mm/hour |\n",
    "| | I_15_50 | 15-minute rainfall intensity threshold at the 50% probability level |\n",
    "| | I_30_75 | 30-minute rainfall intensity threshold at the 75% probability level |\n",
    "| **Watershed** | | |\n",
    "| | Area | Total catchment area |\n",
    "| | BurnRatio | The proportion of catchment area that is burned |\n",
    "| **Model Inputs** | | |\n",
    "| | Terrain_M1 | Terrain variable for the M1 model |\n",
    "| | Fire_M1 | Fire variable for the M1 model |\n",
    "| | Soil_M1 | Soil variable for the M1 model |\n",
    "\n",
    "Once again, we emphasize that these variables and names are arbitrary. In your own code, you can export any variables you like, and use any field names you like.\n",
    "\n",
    "Before creating the properties dict, we'll need to filter our watershed variables. This is because we calculated these variables for the initial stream network, which had many more segments than the final network. Here, we can reuse the `keep` indices to filter the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef088ba-4300-47f1-b748-d7fb34945619",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_km2 = area_km2[keep]\n",
    "burn_ratio = burn_ratio[keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d78e3-e114-4fd9-b99e-ef362243c702",
   "metadata": {},
   "source": [
    "We can now build the properties data array. Here, the keys are the names of the data fields in the exported file. Each set of values should be a vector with one element per stream segment, so we will need to index the results arrays to extract relevant results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93b18c-62da-4a5e-80af-b9c1721317da",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = {\n",
    "    # Results\n",
    "    \"H_24\": hazards[:,1],\n",
    "    \"H_35\": hazards[:,2],\n",
    "    \"H_40\": hazards[:,3],\n",
    "    \"P_35\": likelihoods[:,1],\n",
    "    \"V_35\": volumes[:,1],\n",
    "    \"I_15_50\": intensities[:,0,1],\n",
    "    \"I_30_75\": intensities[:,1,2],\n",
    "    # Watershed\n",
    "    \"Area\": area_km2,\n",
    "    \"BurnRatio\": burn_ratio,\n",
    "    # Model Inputs\n",
    "    \"Terrain_M1\": T,\n",
    "    \"Fire_M1\": F,\n",
    "    \"Soil_M1\": S,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1194d9d1-e963-4d7f-aa07-7b8796b1d60f",
   "metadata": {},
   "source": [
    "We can now use the `save` command to save a Shapefile with the segments and their data fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404a9de-4d53-4f84-9092-06480e6efa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = segments.save(exports/\"segments.shp\", properties=properties, overwrite=True)\n",
    "print_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f151b1fb-a516-4ccc-8155-5752d15d7b21",
   "metadata": {},
   "source": [
    "As a reminder, the segments have LineString geometries and resemble the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28278faf-adac-41d2-bb1b-b2daa46e2bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.network(segments, title='Segments')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fdf702-ef68-4d53-915d-84279a3f0d65",
   "metadata": {},
   "source": [
    "### Basins\n",
    "We'll next export the outlet basins. The basins have Polygon geometries, and each corresponds to the catchment area for one of the local drainage networks. Most networks will have fewer basins than segments, as there are usually multiple segments in each local drainage network. To export the basins, we'll use the `save` command again, but this time we'll set the `type` option to basins:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee46d9a-765c-4823-9e39-d95d9a1b655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = segments.save(\n",
    "    exports/\"basins.shp\", type=\"basins\", properties=properties, overwrite=True\n",
    ")\n",
    "print_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7a4a1b-1155-40b4-83ac-1270fe331200",
   "metadata": {},
   "source": [
    "Let's plot the basins against the segments. Here, stream segments are blue lines, and basins are pink polygons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277084ab-0434-4539-b374-63439d7a736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(plot)\n",
    "plot.network(segments, title='Segments + Basins', show_basins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bfa726-bdd0-4e6b-8066-1100d00f6fcf",
   "metadata": {},
   "source": [
    "### Outlets\n",
    "Finally, we'll export the basin outlets (sometimes referred to as *pour points*). The outlets have Point geometries, and there is one outlet per basin. Essentially, an outlet is the point in a basin where all flow paths will eventually meet.\n",
    "\n",
    "Before exporting the outlets, we'll want to remove any [nested drainage basins](https://ghsc.code-pages.usgs.gov/lhp/pfdf/guide/segments/filter.html#nested-basins) from the network. A nested basin is an outlet basin that is fully contained within another basin. Nested basins occur when the network breaks [flow continuity](https://ghsc.code-pages.usgs.gov/lhp/pfdf/guide/segments/filter.html#flow-continuity), and they result in undesirable \"hanging\" outlet points in the exported dataset. We'll use the `isnested` command to identify segments in nested basins, and then remove them with the `remove` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63627daa-360b-4526-8f18-292e9c44f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested = segments.isnested()\n",
    "segments.remove(nested)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7173c110-2ec0-4451-a3b8-cb97f9fde44c",
   "metadata": {},
   "source": [
    "When we export the outlets, we're not going to include any data properties in the Shapefile. This is usually best practice for outlets, because some outlets can occupy the same spatial point. This occurs, when two basins end at a confluence point - essentially, one outlet is assigned to each of the merging basins, and the two outlets overlap. This can cause confusion when inspecting data fields, as users may unknowingly inspect the wrong outlet for a merging basin. Consequently, we'll export the outlets without data properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99934563-683d-409f-9489-a177d9221ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = segments.save(exports/\"outlets.shp\", type=\"outlets\", overwrite=True)\n",
    "print_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401c02f9-0f20-4ff0-9531-99b244df2456",
   "metadata": {},
   "source": [
    "Let's plot the outlets with the segments and the basins. Here, the outlets are black circles, segments are blue lines, and basins are pink polygons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b8cd2-a3f6-47d0-8add-64c2c7e7fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.network(\n",
    "    segments, title='Segments + Basins + Outlets', show_basins=True, show_outlets=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b0b335-dd7f-4371-a122-7f6a239e01d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Conclusion\n",
    "In this tutorial, we've learned how to use pfdf to implement a hazard assessment. We started by characterizing a watershed - locating burn severity masks, finding flow directions, and computing flow accumulations. We then built a delineation mask and created an initial stream segment network. After characterizing the segments, we refined the network, and we then applied hazards models to each remaining segment. Finally, we exported our assessment results for the segments, basins, and outlets.\n",
    "\n",
    "This concludes the main tutorial series. At this point, you should have a good grasp of pfdf's key components, and are probably ready to start writing code. You can find more in-depth discussion of specific components in the advanced tutorials and [User Guide](https://ghsc.code-pages.usgs.gov/lhp/pfdf/guide/index.html), and refer also to the [API](https://ghsc.code-pages.usgs.gov/lhp/pfdf/api/index.html) for a complete reference guide to pfdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8792acef-602e-4d10-a747-80a8cc43439c",
   "metadata": {},
   "source": [
    "## Quick Reference\n",
    "This script collects the commands used in the tutorials as a quick reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c88ce1-afc7-419a-bdf2-a998ff4b0718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets this notebook for the script\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575fbb56-be65-48b9-b940-177fd618c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfdf.raster import Raster\n",
    "from pfdf import severity, watershed\n",
    "from pfdf.segments import Segments\n",
    "from pfdf.models import g14, s17, c10\n",
    "from pfdf.utils import intensity\n",
    "\n",
    "#####\n",
    "# Parameters\n",
    "#####\n",
    "\n",
    "# Network delineation\n",
    "min_area_km2 = 0.025\n",
    "min_burned_area_km2 = 0.01\n",
    "max_length_m = 500\n",
    "\n",
    "# Network filtering\n",
    "min_burn_ratio = 0.25\n",
    "min_slope = 0.12\n",
    "max_area_km2 = 8\n",
    "max_developed_area_km2 = 0.025\n",
    "max_confinement = 174\n",
    "neighborhood = 4\n",
    "\n",
    "# Hazard modeling\n",
    "I15_mm_hr = [16, 24, 35, 40]\n",
    "durations = [15, 30, 60]\n",
    "probabilities = [0.5, 0.75]\n",
    "\n",
    "#####\n",
    "# Assessment\n",
    "#####\n",
    "\n",
    "# Load datasets\n",
    "perimeter = Raster.from_file('preprocessed/perimeter.tif', isbool=True).values\n",
    "dem = Raster.from_file('preprocessed/dem.tif')\n",
    "dnbr = Raster.from_file('preprocessed/dnbr.tif')\n",
    "kf = Raster.from_file('preprocessed/kf.tif')\n",
    "barc4 = Raster.from_file('preprocessed/barc4.tif')\n",
    "iswater = Raster.from_file('preprocessed/iswater.tif', isbool=True).values\n",
    "isdeveloped = Raster.from_file('preprocessed/isdeveloped.tif', isbool=True)\n",
    "isretainment = Raster.from_file('preprocessed/retainments.tif', isbool=True)\n",
    "\n",
    "# Severity masks\n",
    "isburned = severity.mask(barc4, \"burned\")\n",
    "moderate_high = severity.mask(barc4, [\"moderate\", \"high\"])\n",
    "\n",
    "# Watershed characteristics\n",
    "conditioned = watershed.condition(dem)\n",
    "flow = watershed.flow(conditioned)\n",
    "slopes = watershed.slopes(conditioned, flow)\n",
    "relief = watershed.relief(conditioned, flow)\n",
    "\n",
    "# Flow accumulations\n",
    "pixel_area = dem.pixel_area(units='kilometers')\n",
    "area = watershed.accumulation(flow, times=pixel_area)\n",
    "burned_area = watershed.accumulation(flow, mask=isburned, times=pixel_area)\n",
    "nretainments = watershed.accumulation(flow, mask=isretainment)\n",
    "\n",
    "# Delineation mask\n",
    "large_enough = area.values >= min_area_km2\n",
    "below_burn = burned_area.values >= min_burned_area_km2\n",
    "below_retainment = nretainments.values > 0\n",
    "at_risk = perimeter | below_burn\n",
    "mask = large_enough & at_risk & ~iswater & ~below_retainment\n",
    "\n",
    "# Delineate initial network\n",
    "segments = Segments(flow, mask, max_length_m)\n",
    "\n",
    "# Compute segment characteristics\n",
    "area_km2 = segments.area(units='kilometers')\n",
    "burn_ratio = segments.burn_ratio(isburned)\n",
    "slope = segments.slope(slopes)\n",
    "confinement = segments.confinement(dem, neighborhood)\n",
    "developed_area_km2 = segments.developed_area(isdeveloped)\n",
    "in_perimeter = segments.in_perimeter(perimeter)\n",
    "\n",
    "# Classify segments\n",
    "floodlike = area_km2 > max_area_km2\n",
    "burned = burn_ratio >= min_burn_ratio\n",
    "steep = slope >= min_slope\n",
    "confined = confinement <= max_confinement\n",
    "undeveloped = developed_area_km2 <= max_developed_area_km2\n",
    "\n",
    "# Determine segments that should be retained\n",
    "at_risk = burned & steep & confined & undeveloped\n",
    "keep = ~floodlike & (in_perimeter | at_risk)\n",
    "keep = segments.continuous(keep)\n",
    "\n",
    "# Filter the netowrk\n",
    "segments.keep(keep)\n",
    "\n",
    "# Volume model\n",
    "Bmh_km2 = segments.burned_area(moderate_high)\n",
    "relief = segments.relief(relief)\n",
    "volumes, Vmin, Vmax = g14.emergency(I15_mm_hr, Bmh_km2, relief)\n",
    "\n",
    "# Likelihood model\n",
    "B, Ct, Cf, Cs = s17.M1.parameters(durations=15)\n",
    "R15 = intensity.to_accumulation(I15_mm_hr, durations=15)\n",
    "T, F, S = s17.M1.variables(segments, moderate_high, slopes, dnbr, kf, omitnan=True)\n",
    "likelihoods = s17.likelihood(R15, B, Ct, T, Cf, F, Cs, S)\n",
    "\n",
    "# Combined hazard classification\n",
    "hazards = c10.hazard(\n",
    "    likelihoods, volumes, p_thresholds=[0.2, 0.4, 0.6, 0.8]\n",
    ")\n",
    "\n",
    "# Rainfall thresholds\n",
    "B, Ct, Cf, Cs = s17.M1.parameters(durations=durations)\n",
    "accumulations = s17.accumulation(probabilities, B, Ct, T, Cf, F, Cs, S)\n",
    "intensities = intensity.from_accumulation(accumulations, durations=durations)\n",
    "\n",
    "#####\n",
    "# Export\n",
    "#####\n",
    "\n",
    "# Filter watershed variables\n",
    "area_km2 = area_km2[keep]\n",
    "burn_ratio = burn_ratio[keep]\n",
    "\n",
    "# Build property dict\n",
    "properties = {\n",
    "    # Results\n",
    "    \"H_24\": hazards[:,1],\n",
    "    \"H_35\": hazards[:,2],\n",
    "    \"H_40\": hazards[:,3],\n",
    "    \"P_35\": likelihoods[:,1],\n",
    "    \"V_35\": volumes[:,1],\n",
    "    \"I_15_50\": intensities[:,0,1],\n",
    "    \"I_30_75\": intensities[:,1,2],\n",
    "    # Watershed\n",
    "    \"Area\": area_km2,\n",
    "    \"BurnRatio\": burn_ratio,\n",
    "    # Model Inputs\n",
    "    \"Terrain_M1\": T,\n",
    "    \"Fire_M1\": F,\n",
    "    \"Soil_M1\": S,\n",
    "}\n",
    "\n",
    "# Export segments\n",
    "path = segments.save(\"exports/segments.shp\", properties=properties, overwrite=True)\n",
    "\n",
    "# Export basins\n",
    "path = segments.save(\n",
    "    \"exports/basins.shp\", type=\"basins\", properties=properties, overwrite=True\n",
    ")\n",
    "\n",
    "# Export outlets\n",
    "nested = segments.isnested()\n",
    "segments.remove(nested)\n",
    "path = segments.save(\"exports/outlets.shp\", type=\"outlets\", overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}